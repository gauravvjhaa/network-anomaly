{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13733777,"sourceType":"datasetVersion","datasetId":8738186}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\nHIKARI-2021 Advanced Anomaly Detection (Full Debug Version)\n\nFeatures:\n 1. Clean robust preprocessing + per-feature scaling\n 2. AE residual modeling (Mahalanobis, GMM PCA, optional Flow)\n 3. Per-cluster latent thresholds\n 4. Latent-space supervised calibrated classifier\n 5. Ensemble (AE hybrid, GMM, IF, XGB, latent LR, optional flow) + meta logistic regression\n 6. PR-based threshold selection\n 7. Extensive DEBUG instrumentation (shapes, stats, timings, NaN checks)\n\nAdjust verbosity:\n  DEBUG_LEVEL: 0 (quiet), 1 (steps), 2 (detailed)\n  LOG_BATCHES: per-batch AE training logs (may be noisy)\n\"\"\"\n\n# ======================\n# Configuration Flags\n# ======================\n\nDEBUG_LEVEL = 2              # 0,1,2\nLOG_BATCHES = False          # Per-batch logs inside AE training\nCSV_PATH = \"/kaggle/input/hikari-dataset/ALLFLOWMETER_HIKARI2021.csv\"\nSMALL_SAMPLE = None          # e.g. 300000 for quicker dev runs\nN_SPLITS = 3\nBATCH_SIZE = 256\nEPOCHS = 60\nCLIP_VALUE = 6.0\nPR_TARGET_PRECISION = 0.95\nKMEANS_CLUSTERS = 6\nGMM_COMPONENTS = 3\nPCA_RESID_DIM = 12\nLATENT_DIM = 32\nAE_WIDTH = 128\nDROPOUT = 0.25\nLEARNING_RATE = 1e-3\nISOLATION_TREES = 300\nXGB_PARAMS = dict(\n    n_estimators=400, max_depth=8, learning_rate=0.05,\n    subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0,\n    objective=\"binary:logistic\", eval_metric=\"logloss\", random_state=42\n)\n\nENABLE_FLOW = False          # Normalizing flow off by default\nFLOW_STEPS = 400\nFLOW_LR = 3e-4\nFLOW_CLIPNORM = 1.0\nFLOW_HIDDEN = 64\nFLOW_BIJECTORS = 4\nFLOW_CLIP_INPUT = 5.0\n\nPLOT = True\n\n# ======================\n# Imports\n# ======================\n\nimport os, gc, time, psutil, math\nimport numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    precision_recall_curve, average_precision_score, roc_auc_score,\n    classification_report, confusion_matrix\n)\nfrom sklearn.decomposition import PCA\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.cluster import KMeans\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.calibration import CalibratedClassifierCV\nimport xgboost as xgb\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, Model, Input\nfrom tensorflow.keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau\n\ntry:\n    import tensorflow_probability as tfp\n    TFP_AVAILABLE = True\n    tfd = tfp.distributions\n    tfb = tfp.bijectors\nexcept Exception:\n    TFP_AVAILABLE = False\n\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# ======================\n# Debug Utilities\n# ======================\n\nSTART_TIME = time.time()\n\ndef mem_usage_mb():\n    return psutil.Process(os.getpid()).memory_info().rss / (1024**2)\n\ndef dbg(msg, level=1):\n    if DEBUG_LEVEL >= level:\n        elapsed = time.time() - START_TIME\n        print(f\"[{elapsed:7.1f}s | {mem_usage_mb():7.1f}MB] {msg}\")\n\ndef banner(title):\n    line = \"=\" * min(100, max(30, len(title)+10))\n    dbg(f\"\\n{line}\\n{title}\\n{line}\", level=1)\n\ndef stats_vector(name, arr, level=2):\n    if DEBUG_LEVEL >= level:\n        if arr is None:\n            dbg(f\"{name}: None\", level)\n            return\n        dbg(f\"{name}: shape={arr.shape}, dtype={arr.dtype}, \"\n            f\"min={np.nanmin(arr):.4f}, max={np.nanmax(arr):.4f}, \"\n            f\"mean={np.nanmean(arr):.4f}, std={np.nanstd(arr):.4f}, \"\n            f\"NaNs={np.isnan(arr).sum()}\")\n\n# ======================\n# Helper Functions\n# ======================\n\ndef detect_label_column(df: pd.DataFrame) -> str:\n    candidates = ['label', 'Label', 'LABEL', 'class', 'Class', 'attack', 'Attack', 'type', 'Type', 'TYPE']\n    for c in candidates:\n        if c in df.columns:\n            return c\n    for c in df.columns:\n        if any(k in c.lower() for k in ['label','class','attack','type']):\n            return c\n    return df.columns[-1]\n\ndef make_binary_labels(df: pd.DataFrame, label_col: str) -> pd.Series:\n    if label_col != 'label':\n        df.rename(columns={label_col: 'label'}, inplace=True)\n    s = df['label'].astype(str).str.lower().str.strip()\n    normal_keywords = ['normal','benign','legitimate','background']\n    is_normal = None\n    for kw in normal_keywords:\n        if s.str.contains(kw).any():\n            is_normal = s.str.contains(kw)\n            dbg(f\"Detected normal keyword: {kw}\")\n            break\n    if is_normal is None:\n        maj = df['label'].value_counts().idxmax()\n        dbg(f\"No explicit normal token; using majority label '{maj}' as normal.\")\n        is_normal = (df['label'] == maj)\n    return (~is_normal).astype(int)\n\ndef select_numeric_features(df: pd.DataFrame) -> list:\n    exclude_patterns = ['label','ip','port','time','mac','src','dst','flow','address','id']\n    exclude_cols = set(['label','is_attack'])\n    for col in df.columns:\n        lc = col.lower()\n        if any(p in lc for p in exclude_patterns):\n            exclude_cols.add(col)\n        if df[col].dtype not in [np.int16,np.int32,np.int64,np.float16,np.float32,np.float64]:\n            exclude_cols.add(col)\n    feats = [c for c in df.columns if c not in exclude_cols]\n    dbg(f\"Numeric feature selection -> {len(feats)} columns\")\n    return feats\n\ndef robust_clean(df: pd.DataFrame, features: list) -> list:\n    dbg(\"Starting robust clean on features...\")\n    df[features] = df[features].replace([np.inf,-np.inf], np.nan)\n    nan_before = df[features].isna().sum().sum()\n    dbg(f\"NaNs before fill: {nan_before}\")\n    for c in features:\n        med = df[c].median()\n        if pd.isna(med):\n            med = 0.0\n        df[c] = df[c].fillna(med)\n    nan_after = df[features].isna().sum().sum()\n    dbg(f\"NaNs after fill: {nan_after}\")\n    const = [c for c in features if df[c].nunique() <= 1]\n    if const:\n        dbg(f\"Removing {len(const)} constant features: {const[:10]}{'...' if len(const)>10 else ''}\")\n        features = [c for c in features if c not in const]\n    return features\n\nclass DebugTrainingCallback(Callback):\n    def on_train_begin(self, logs=None):\n        dbg(\"AE training started\", 1)\n    def on_epoch_end(self, epoch, logs=None):\n        dbg(f\"Epoch {epoch+1:03d}/{EPOCHS} \"\n            f\"loss={logs.get('loss'):.6f} val_loss={logs.get('val_loss'):.6f} \"\n            f\"mae={logs.get('mae'):.6f} val_mae={logs.get('val_mae'):.6f}\", 2)\n    def on_train_end(self, logs=None):\n        dbg(\"AE training finished\", 1)\n    def on_batch_end(self, batch, logs=None):\n        if LOG_BATCHES:\n            dbg(f\"  Batch {batch} loss={logs.get('loss'):.6f}\", 2)\n\ndef build_autoencoder(input_dim: int, latent_dim: int = LATENT_DIM, width: int = AE_WIDTH, dropout: float = DROPOUT):\n    dbg(f\"Building AE (input_dim={input_dim}, latent_dim={latent_dim}, width={width}, dropout={dropout})\", 1)\n    inp = Input(shape=(input_dim,), name=\"ae_input\")\n    x = layers.Dense(width, activation='relu')(inp)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(dropout)(x)\n    x = layers.Dense(latent_dim, activation='relu', name=\"latent\")(x)\n    x = layers.BatchNormalization()(x)\n\n    x = layers.Dense(width, activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(dropout)(x)\n    out = layers.Dense(input_dim, activation='linear')(x)\n    ae = Model(inp, out, name=\"Autoencoder\")\n    enc = Model(inp, ae.get_layer('latent').output, name=\"Encoder\")\n    dbg(\"AE built successfully.\", 1)\n    return ae, enc\n\ndef fit_gmm_on_residuals(residuals: np.ndarray, pca_dim: int = PCA_RESID_DIM, n_components: int = GMM_COMPONENTS):\n    dbg(f\"Fitting GMM: residuals shape={residuals.shape}, PCA_dim={pca_dim}, components={n_components}\", 1)\n    pca_dim_eff = min(pca_dim, max(2, residuals.shape[1]-1))\n    pca = PCA(n_components=pca_dim_eff, random_state=42)\n    Z = pca.fit_transform(residuals)\n    dbg(f\"PCA residuals shape: {Z.shape}\", 2)\n    gmm = GaussianMixture(n_components=n_components, covariance_type='full', random_state=42)\n    gmm.fit(Z)\n    dbg(\"GMM fitted.\", 1)\n    return pca, gmm\n\ndef mahalanobis_on_residuals(residuals: np.ndarray):\n    dbg(f\"Computing Mahalanobis parameters for residuals shape={residuals.shape}\", 1)\n    cov = np.cov(residuals.T)\n    eps = 1e-6\n    cov += np.eye(cov.shape[0]) * eps\n    inv = np.linalg.inv(cov)\n    mean = residuals.mean(axis=0)\n    dbg(\"Mahalanobis prepared (cov regularized).\", 2)\n    def md(X):\n        diff = X - mean\n        return np.sqrt(np.einsum('ij,jk,ik->i', diff, inv, diff))\n    return md\n\ndef pr_threshold(y_true, scores, target_precision: float = None):\n    dbg(\"Computing PR-based threshold...\", 1)\n    prec, rec, thr = precision_recall_curve(y_true, scores)\n    f1 = (2*prec*rec)/(prec+rec+1e-12)\n    best_idx = np.nanargmax(f1)\n    best_thr = thr[max(best_idx-1,0)]\n    result = dict(kind=\"max_f1\", threshold=float(best_thr),\n                  precision=float(prec[best_idx]), recall=float(rec[best_idx]), f1=float(f1[best_idx]))\n    if target_precision is None:\n        dbg(f\"Max-F1 threshold selected: {result}\", 2)\n        return result\n    ok = np.where(prec[:-1] >= target_precision)[0]\n    if len(ok)==0:\n        dbg(\"Target precision not reachable; fallback to max F1\", 1)\n        dbg(f\"Max-F1: {result}\", 2)\n        return result\n    idx = ok[np.argmax(rec[ok])]\n    result_tp = dict(kind=f\"target_precision_{target_precision}\",\n                     threshold=float(thr[idx]),\n                     precision=float(prec[idx]),\n                     recall=float(rec[idx]),\n                     f1=float(f1[idx]))\n    dbg(f\"Target-precision threshold selected: {result_tp}\", 2)\n    return result_tp\n\ndef evaluate_metrics(y_true, y_pred, prob_scores=None, title=\"\"):\n    acc = accuracy_score(y_true, y_pred)\n    prec = precision_score(y_true, y_pred, zero_division=0)\n    rec = recall_score(y_true, y_pred, zero_division=0)\n    f1 = f1_score(y_true, y_pred, zero_division=0)\n    ap = average_precision_score(y_true, prob_scores) if prob_scores is not None else np.nan\n    auc = roc_auc_score(y_true, prob_scores) if prob_scores is not None else np.nan\n    dbg(f\"[{title}] Acc={acc:.4f} Prec={prec:.4f} Rec={rec:.4f} F1={f1:.4f} AP={ap:.4f} AUC={auc:.4f}\", 1)\n    return dict(accuracy=acc, precision=prec, recall=rec, f1=f1, ap=ap, auc=auc)\n\n# Flow utilities (robust)\ndef standardize_clip(X, mean=None, std=None, clip=FLOW_CLIP_INPUT):\n    if mean is None:\n        mean = X.mean(axis=0)\n    if std is None:\n        std = X.std(axis=0) + 1e-6\n    Z = (X - mean) / std\n    Z = np.clip(Z, -clip, clip)\n    return Z, mean, std\n\ndef try_normalizing_flow(train_residuals: np.ndarray):\n    if not (ENABLE_FLOW and TFP_AVAILABLE):\n        dbg(\"Flow disabled or TFP unavailable.\", 1)\n        return None\n    dbg(\"Initializing normalizing flow...\", 1)\n    Z, m, s = standardize_clip(train_residuals.astype(np.float32))\n    dims = Z.shape[1]\n    bijectors = []\n    for _ in range(FLOW_BIJECTORS):\n        bijectors.append(\n            tfb.MaskedAutoregressiveFlow(\n                shift_and_log_scale_fn=tfb.AutoregressiveNetwork(\n                    params=2, hidden_units=[FLOW_HIDDEN, FLOW_HIDDEN]\n                )\n            )\n        )\n        bijectors.append(tfb.Permute(permutation=list(reversed(range(dims)))))\n    flow_bijector = tfb.Chain(list(reversed(bijectors)))\n    base = tfd.MultivariateNormalDiag(loc=tf.zeros(dims), scale_diag=tf.ones(dims))\n    flow = tfd.TransformedDistribution(distribution=base, bijector=flow_bijector)\n\n    x = tf.convert_to_tensor(Z, dtype=tf.float32)\n    ds = tf.data.Dataset.from_tensor_slices(x).shuffle(8192, seed=42).batch(512).prefetch(tf.data.AUTOTUNE)\n    opt = tf.keras.optimizers.Adam(FLOW_LR, clipnorm=FLOW_CLIPNORM)\n    best = np.inf\n    patience = 0\n    for step, batch in enumerate(ds.repeat(), start=1):\n        with tf.GradientTape() as tape:\n            loss = -tf.reduce_mean(flow.log_prob(batch))\n        if not tf.reduce_all(tf.math.is_finite(loss)):\n            dbg(f\"[Flow] Non-finite loss at step {step}, aborting flow.\", 1)\n            return None\n        grads = tape.gradient(loss, flow.trainable_variables)\n        opt.apply_gradients(zip(grads, flow.trainable_variables))\n        if step % 100 == 0:\n            curr = float(loss.numpy())\n            dbg(f\"[Flow] step {step} loss={curr:.6f}\", 2)\n            if curr + 1e-4 < best:\n                best = curr\n                patience = 0\n            else:\n                patience += 1\n            if patience >= 5 or step >= FLOW_STEPS:\n                break\n    dbg(\"Flow training complete.\", 1)\n    return dict(flow=flow, mean=m.astype(np.float32), std=s.astype(np.float32))\n\ndef score_with_flow(bundle, residuals: np.ndarray):\n    if bundle is None:\n        return None\n    Z, _, _ = standardize_clip(residuals.astype(np.float32),\n                               mean=bundle['mean'], std=bundle['std'])\n    flow = bundle['flow']\n    ll = flow.log_prob(tf.convert_to_tensor(Z)).numpy()\n    return -ll  # negative log-likelihood as anomaly score\n\n# ======================\n# Load dataset\n# ======================\nbanner(\"Loading dataset\")\ndf = pd.read_csv(CSV_PATH)\ndbg(f\"Initial shape: {df.shape}\")\n\nif SMALL_SAMPLE is not None and len(df) > SMALL_SAMPLE:\n    df = df.sample(SMALL_SAMPLE, random_state=42).reset_index(drop=True)\n    dbg(f\"After sampling shape: {df.shape}\")\n\nlabel_col = detect_label_column(df)\ndbg(f\"Detected label column: {label_col}\")\ndf['is_attack'] = make_binary_labels(df, label_col)\ndbg(\"Binary label distribution:\\n\" + str(df['is_attack'].value_counts()))\n\nfeature_cols = select_numeric_features(df)\nfeature_cols = robust_clean(df, feature_cols)\ndbg(f\"Final feature list length: {len(feature_cols)}\")\n\n# ======================\n# Train / Test split\n# ======================\nbanner(\"Train/Test Split\")\nX_all = df[feature_cols].values\ny_all = df['is_attack'].values\nX_train, X_test, y_train, y_test = train_test_split(\n    X_all, y_all, test_size=0.25, random_state=42, stratify=y_all\n)\ndbg(f\"X_train={X_train.shape}, X_test={X_test.shape}\")\ndbg(f\"Train class distribution: {np.bincount(y_train)} | Test: {np.bincount(y_test)}\")\n\n# AE normal-only\nX_train_norm = X_train[y_train==0]\ndbg(f\"Normal-only count for AE: {X_train_norm.shape[0]}\")\n\n# Scalers\nscaler_ae = StandardScaler()\nX_train_norm_s = scaler_ae.fit_transform(X_train_norm)\nX_train_norm_s = np.clip(X_train_norm_s, -CLIP_VALUE, CLIP_VALUE)\nstats_vector(\"X_train_norm_s\", X_train_norm_s)\n\nX_tr_ae, X_val_ae = train_test_split(X_train_norm_s, test_size=0.1, random_state=42)\ndbg(f\"AE train subset: {X_tr_ae.shape}, AE val subset: {X_val_ae.shape}\")\n\nscaler_sup = StandardScaler()\nX_train_sup_s = scaler_sup.fit_transform(X_train)\nX_test_sup_s = scaler_sup.transform(X_test)\nstats_vector(\"X_train_sup_s\", X_train_sup_s, 2)\n\n# ======================\n# Cross-validation (Meta Ensemble)\n# ======================\nbanner(\"Cross-Validation Meta Ensemble\")\nskf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\noof_scores = []\noof_y = []\n\nfor fold,(tr_idx,val_idx) in enumerate(skf.split(X_train,y_train), start=1):\n    banner(f\"Fold {fold}/{N_SPLITS}\")\n    X_tr, X_val = X_train[tr_idx], X_train[val_idx]\n    y_tr, y_val = y_train[tr_idx], y_train[val_idx]\n    dbg(f\"Fold train shape={X_tr.shape}, val shape={X_val.shape}\")\n    dbg(f\"Fold class dist train={np.bincount(y_tr)}, val={np.bincount(y_val)}\")\n\n    # Fold AE normals\n    X_tr_norm = X_tr[y_tr==0]\n    X_tr_norm_s = scaler_ae.fit_transform(X_tr_norm)\n    X_tr_norm_s = np.clip(X_tr_norm_s, -CLIP_VALUE, CLIP_VALUE)\n    X_tr_ae_f, X_v_ae_f = train_test_split(X_tr_norm_s, test_size=0.1, random_state=42)\n    dbg(f\"Fold AE normal train={X_tr_ae_f.shape}, normal val={X_v_ae_f.shape}\")\n\n    # Build AE\n    ae, encoder = build_autoencoder(X_tr_norm_s.shape[1], LATENT_DIM, AE_WIDTH, DROPOUT)\n    ae.compile(optimizer=keras.optimizers.Adam(LEARNING_RATE, clipnorm=1.0),\n               loss='mse', metrics=['mae'])\n    callbacks = [\n        DebugTrainingCallback(),\n        EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True, verbose=0),\n        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-5, verbose=0)\n    ]\n    t0 = time.time()\n    ae.fit(X_tr_ae_f, X_tr_ae_f,\n           validation_data=(X_v_ae_f, X_v_ae_f),\n           epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=0,\n           callbacks=callbacks)\n    dbg(f\"AE fold training time: {time.time()-t0:.1f}s\")\n\n    # Validation residuals\n    X_val_ae_s = scaler_ae.transform(X_val)\n    X_val_ae_s = np.clip(X_val_ae_s, -CLIP_VALUE, CLIP_VALUE)\n    X_val_recon = ae.predict(X_val_ae_s, verbose=0)\n    val_residuals = X_val_ae_s - X_val_recon\n    recon_mse = np.mean(val_residuals**2, axis=1)\n    stats_vector(\"val_residuals\", val_residuals, 2)\n    stats_vector(\"recon_mse\", recon_mse, 2)\n\n    # Train residual stats on fold normals\n    X_tr_recon = ae.predict(X_tr_norm_s, verbose=0)\n    tr_residuals = X_tr_norm_s - X_tr_recon\n\n    md_fn = mahalanobis_on_residuals(tr_residuals)\n    md_score_val = md_fn(val_residuals)\n    stats_vector(\"md_score_val\", md_score_val, 2)\n\n    pca_resid, gmm = fit_gmm_on_residuals(tr_residuals,\n                                          pca_dim=PCA_RESID_DIM,\n                                          n_components=GMM_COMPONENTS)\n    val_resid_pca = pca_resid.transform(val_residuals)\n    gmm_score_val = -gmm.score_samples(val_resid_pca)\n    stats_vector(\"gmm_score_val\", gmm_score_val, 2)\n\n    flow_score_val = None\n    if ENABLE_FLOW and TFP_AVAILABLE:\n        flow_bundle = try_normalizing_flow(tr_residuals)\n        if flow_bundle is not None:\n            flow_score_val = score_with_flow(flow_bundle, val_residuals)\n            stats_vector(\"flow_score_val\", flow_score_val, 2)\n        else:\n            dbg(\"Flow skipped this fold (instability or disabled).\",1)\n\n    # Latent embeddings\n    X_val_lat = encoder.predict(X_val_ae_s, verbose=0)\n    stats_vector(\"X_val_lat\", X_val_lat, 2)\n\n    X_tr_ae_sup_s = scaler_ae.transform(X_tr)\n    X_tr_ae_sup_s = np.clip(X_tr_ae_sup_s, -CLIP_VALUE, CLIP_VALUE)\n    X_tr_lat = encoder.predict(X_tr_ae_sup_s, verbose=0)\n    logreg = LogisticRegression(max_iter=200, class_weight='balanced')\n    logreg_cal = CalibratedClassifierCV(logreg, method='sigmoid', cv=3)\n    logreg_cal.fit(X_tr_lat, y_tr)\n    lat_prob_val = logreg_cal.predict_proba(X_val_lat)[:, 1]\n    stats_vector(\"lat_prob_val\", lat_prob_val, 2)\n\n    # Isolation Forest & XGB (on original features)\n    iso = IsolationForest(n_estimators=ISOLATION_TREES, contamination='auto', random_state=42)\n    iso.fit(X_tr)\n    iso_score_val = -iso.score_samples(X_val)\n    stats_vector(\"iso_score_val\", iso_score_val, 2)\n\n    pos_weight = (y_tr==0).sum()/max(1,(y_tr==1).sum())\n    dbg(f\"XGB pos_weight={pos_weight:.4f}\", 2)\n    xgb_model = xgb.XGBClassifier(**{**XGB_PARAMS, \"scale_pos_weight\": pos_weight})\n    scaler_fold_sup = StandardScaler().fit(X_tr)\n    X_tr_fold_sup_s = scaler_fold_sup.transform(X_tr)\n    X_val_fold_sup_s = scaler_fold_sup.transform(X_val)\n    xgb_model.fit(X_tr_fold_sup_s, y_tr)\n    xgb_prob_val = xgb_model.predict_proba(X_val_fold_sup_s)[:,1]\n    stats_vector(\"xgb_prob_val\", xgb_prob_val, 2)\n\n    # Hybrid AE score\n    mse_s = (recon_mse - recon_mse.mean())/(recon_mse.std()+1e-9)\n    md_s  = (md_score_val - md_score_val.mean())/(md_score_val.std()+1e-9)\n    hybrid_val = 0.6*mse_s + 0.4*md_s\n    stats_vector(\"hybrid_val\", hybrid_val, 2)\n\n    # Per-cluster thresholds\n    tr_lat_norm = encoder.predict(X_tr_norm_s, verbose=0)\n    kmeans = KMeans(n_clusters=KMEANS_CLUSTERS, random_state=42, n_init=10)\n    kmeans.fit(tr_lat_norm)\n    tr_tr_recon = ae.predict(X_tr_norm_s, verbose=0)\n    tr_resid_norm = X_tr_norm_s - tr_tr_recon\n    tr_mse = np.mean(tr_resid_norm**2, axis=1)\n    tr_mse_s = (tr_mse - tr_mse.mean())/(tr_mse.std()+1e-9)\n    tr_md = md_fn(tr_resid_norm)\n    tr_md_s = (tr_md - tr_md.mean())/(tr_md.std()+1e-9)\n    tr_hybrid = 0.6*tr_mse_s + 0.4*tr_md_s\n    tr_clusters = kmeans.predict(tr_lat_norm)\n    for c in range(KMEANS_CLUSTERS):\n        cluster_vals = tr_hybrid[tr_clusters==c]\n        dbg(f\"Cluster {c} count={cluster_vals.size} hybrid_mean={cluster_vals.mean():.4f} hybrid_std={cluster_vals.std():.4f}\", 2)\n    # Not storing thresholds now because final cluster training will re-fit on full data.\n\n    fold_features = {\n        \"ae_hybrid\": hybrid_val,\n        \"gmm\": gmm_score_val,\n        \"iso\": iso_score_val,\n        \"lat_prob\": lat_prob_val,\n        \"xgb_prob\": xgb_prob_val\n    }\n    if flow_score_val is not None:\n        fold_features[\"flow\"] = flow_score_val\n\n    F = np.vstack([fold_features[k] for k in fold_features.keys()]).T\n    oof_scores.append(F)\n    oof_y.append(y_val)\n    dbg(f\"Fold {fold} meta feature matrix shape: {F.shape}\", 2)\n\n# ======================\n# Meta Model Training\n# ======================\nbanner(\"Meta Ensemble Training\")\noof_X = np.vstack(oof_scores)\noof_y = np.concatenate(oof_y)\nstats_vector(\"oof_X\", oof_X, 2)\ndbg(f\"oof_y distribution: {np.bincount(oof_y)}\")\n\nmeta = LogisticRegression(max_iter=200, class_weight='balanced')\nmeta.fit(oof_X, oof_y)\noof_meta_prob = meta.predict_proba(oof_X)[:,1]\nstats_vector(\"oof_meta_prob\", oof_meta_prob, 2)\n\nthr_info = pr_threshold(oof_y, oof_meta_prob, target_precision=PR_TARGET_PRECISION)\nMETA_THRESHOLD = thr_info[\"threshold\"]\ndbg(f\"Meta threshold selected: {thr_info}\", 1)\n\n# ======================\n# Final Fit on Full Train\n# ======================\nbanner(\"Final Model Fit (Full Training)\")\n\nscaler_ae_final = StandardScaler()\nX_train_norm_s_full = scaler_ae_final.fit_transform(X_train[y_train==0])\nX_train_norm_s_full = np.clip(X_train_norm_s_full, -CLIP_VALUE, CLIP_VALUE)\nstats_vector(\"X_train_norm_s_full\", X_train_norm_s_full, 2)\n\nX_tr_ae_full, X_v_ae_full = train_test_split(X_train_norm_s_full, test_size=0.1, random_state=42)\nae_final, encoder_final = build_autoencoder(X_train_norm_s_full.shape[1], LATENT_DIM, AE_WIDTH, DROPOUT)\nae_final.compile(optimizer=keras.optimizers.Adam(LEARNING_RATE, clipnorm=1.0), loss='mse', metrics=['mae'])\nae_final.fit(X_tr_ae_full, X_tr_ae_full,\n             validation_data=(X_v_ae_full, X_v_ae_full),\n             epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=0,\n             callbacks=[DebugTrainingCallback(),\n                        EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True, verbose=0),\n                        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-5, verbose=0)])\n\n# Residual modeling full\nX_tr_recon_full = ae_final.predict(X_train_norm_s_full, verbose=0)\ntr_resid_full = X_train_norm_s_full - X_tr_recon_full\nmd_full = mahalanobis_on_residuals(tr_resid_full)\npca_resid_full, gmm_full = fit_gmm_on_residuals(tr_resid_full, PCA_RESID_DIM, GMM_COMPONENTS)\nflow_full = None\nif ENABLE_FLOW and TFP_AVAILABLE:\n    flow_full = try_normalizing_flow(tr_resid_full)\n\ntr_lat_norm_full = encoder_final.predict(X_train_norm_s_full, verbose=0)\nkmeans_full = KMeans(n_clusters=KMEANS_CLUSTERS, random_state=42, n_init=10)\nkmeans_full.fit(tr_lat_norm_full)\n\ntr_mse_full = np.mean(tr_resid_full**2, axis=1)\ntr_mse_s_full = (tr_mse_full - tr_mse_full.mean())/(tr_mse_full.std()+1e-9)\ntr_md_full_vals = md_full(tr_resid_full)\ntr_md_s_full = (tr_md_full_vals - tr_md_full_vals.mean())/(tr_md_full_vals.std()+1e-9)\ntr_hybrid_full = 0.6*tr_mse_s_full + 0.4*tr_md_s_full\ntr_clusters_full = kmeans_full.predict(tr_lat_norm_full)\ncluster_thresholds_final = {}\nfor c in range(KMEANS_CLUSTERS):\n    cluster_vals = tr_hybrid_full[tr_clusters_full==c]\n    cluster_thresholds_final[c] = np.quantile(cluster_vals, 0.995)\n    dbg(f\"Final cluster {c} threshold={cluster_thresholds_final[c]:.6f} count={cluster_vals.size}\", 2)\n\n# Supervised models full\npos_weight_full = (y_train==0).sum()/max(1,(y_train==1).sum())\ndbg(f\"POS_WEIGHT full={pos_weight_full:.4f}\", 2)\nxgb_full = xgb.XGBClassifier(**{**XGB_PARAMS, \"scale_pos_weight\": pos_weight_full})\nxgb_full.fit(X_train_sup_s, y_train)\n\nX_train_ae_s_full = scaler_ae_final.transform(X_train)\nX_train_ae_s_full = np.clip(X_train_ae_s_full, -CLIP_VALUE, CLIP_VALUE)\nX_train_lat_full = encoder_final.predict(X_train_ae_s_full, verbose=0)\nlogreg_full = LogisticRegression(max_iter=200, class_weight='balanced')\nlogreg_full_cal = CalibratedClassifierCV(logreg_full, method='sigmoid', cv=3)\nlogreg_full_cal.fit(X_train_lat_full, y_train)\n\niso_full = IsolationForest(n_estimators=ISOLATION_TREES, random_state=42, contamination='auto')\niso_full.fit(X_train)\n\n# ======================\n# Inference on Test\n# ======================\nbanner(\"Inference on Test Set\")\nX_test_ae_s_full = scaler_ae_final.transform(X_test)\nX_test_ae_s_full = np.clip(X_test_ae_s_full, -CLIP_VALUE, CLIP_VALUE)\nX_test_recon = ae_final.predict(X_test_ae_s_full, verbose=0)\ntest_resid = X_test_ae_s_full - X_test_recon\ntest_mse = np.mean(test_resid**2, axis=1)\ntest_mse_s = (test_mse - test_mse.mean())/(test_mse.std()+1e-9)\nmd_test_vals = md_full(test_resid)\nmd_test_s = (md_test_vals - md_test_vals.mean())/(md_test_vals.std()+1e-9)\nhybrid_test = 0.6*test_mse_s + 0.4*md_test_s\nstats_vector(\"hybrid_test\", hybrid_test, 2)\n\ntest_resid_pca = pca_resid_full.transform(test_resid)\ngmm_score_test = -gmm_full.score_samples(test_resid_pca)\nflow_score_test = score_with_flow(flow_full, test_resid) if (ENABLE_FLOW and flow_full is not None) else None\n\nX_test_lat = encoder_final.predict(X_test_ae_s_full, verbose=0)\ntest_clusters = kmeans_full.predict(X_test_lat)\nper_cluster_pred = np.array([\n    1 if hybrid_test[i] > cluster_thresholds_final[test_clusters[i]] else 0\n    for i in range(len(X_test))\n])\n\nxgb_prob_test = xgb_full.predict_proba(X_test_sup_s)[:,1]\nlat_prob_test = logreg_full_cal.predict_proba(X_test_lat)[:,1]\niso_score_test = -iso_full.score_samples(X_test)\n\nmeta_parts = {\n    \"ae_hybrid\": hybrid_test,\n    \"gmm\": gmm_score_test,\n    \"iso\": iso_score_test,\n    \"lat_prob\": lat_prob_test,\n    \"xgb_prob\": xgb_prob_test\n}\nif flow_score_test is not None:\n    meta_parts[\"flow\"] = flow_score_test\n\nmeta_X_test = np.vstack([meta_parts[k] for k in meta_parts]).T\nstats_vector(\"meta_X_test\", meta_X_test, 2)\nmeta_prob_test = meta.predict_proba(meta_X_test)[:,1]\nmeta_pred_test = (meta_prob_test >= META_THRESHOLD).astype(int)\n\n# Evaluations\nevaluate_metrics(y_test, per_cluster_pred, prob_scores=hybrid_test, title=\"AE per-cluster\")\nmeta_metrics = evaluate_metrics(y_test, meta_pred_test, prob_scores=meta_prob_test, title=\"Meta Ensemble\")\n\ndbg(\"Classification report (Meta):\\n\" + classification_report(y_test, meta_pred_test, target_names=['Normal','Attack']), 1)\n\ncm = confusion_matrix(y_test, meta_pred_test)\nif PLOT:\n    plt.figure(figsize=(6,5))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                xticklabels=['Normal','Attack'], yticklabels=['Normal','Attack'])\n    plt.title(\"Confusion Matrix - Meta Ensemble\")\n    plt.ylabel(\"True\")\n    plt.xlabel(\"Predicted\")\n    plt.show()\n\n# ======================\n# Benchmark Comparison\n# ======================\nbanner(\"Benchmark Comparison\")\ncomparison = pd.DataFrame({\n    \"Method\": [\n        \"Paper (AE)\", \"Fernandes RF\", \"Fernandes XGB\", \"Vitorino KNN\", \"Vitorino MLP\",\n        \"Our AE per-cluster\", \"Our Meta Ensemble\"\n    ],\n    \"Accuracy\": [94, 98, 96, 98, 90,\n                 accuracy_score(y_test, per_cluster_pred)*100,\n                 meta_metrics['accuracy']*100],\n    \"Precision\": [81, 99, 99, 98, 90,\n                  precision_score(y_test, per_cluster_pred, zero_division=0)*100,\n                  meta_metrics['precision']*100],\n    \"Recall\": [99, 69, 44, 98, 90,\n               recall_score(y_test, per_cluster_pred, zero_division=0)*100,\n               meta_metrics['recall']*100],\n    \"F1\": [89, 81, 61, 98, 89,\n           f1_score(y_test, per_cluster_pred, zero_division=0)*100,\n           meta_metrics['f1']*100],\n}).round(2)\n\ndbg(\"Final comparison table:\\n\" + comparison.to_string(index=False), 1)\n\nprint(\"\\nNotes:\")\nprint(\"- Adjust PR_TARGET_PRECISION to bias threshold toward higher precision.\")\nprint(\"- Increase quantile (0.995) for cluster thresholds for stricter anomaly gating.\")\nprint(\"- ENABLE_FLOW=True may add a new signal; watch logs for stability.\")\nprint(\"- Reduce DEBUG_LEVEL to 1 or 0 for cleaner output once stable.\")\nprint(\"- Use SMALL_SAMPLE to iterate faster on large dataset.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T14:41:14.385455Z","iopub.execute_input":"2025-11-16T14:41:14.385775Z","iopub.status.idle":"2025-11-16T15:01:44.447688Z","shell.execute_reply.started":"2025-11-16T14:41:14.385753Z","shell.execute_reply":"2025-11-16T15:01:44.447074Z"}},"outputs":[{"name":"stdout","text":"[    0.0s | 11849.2MB] \n==============================\nLoading dataset\n==============================\n[    5.0s | 12387.4MB] Initial shape: (555278, 88)\n[    5.0s | 12387.4MB] Detected label column: Label\n[    5.7s | 12430.6MB] No explicit normal token; using majority label '0' as normal.\n[    5.8s | 12430.6MB] Binary label distribution:\nis_attack\n0    517582\n1     37696\nName: count, dtype: int64\n[    5.8s | 12388.8MB] Numeric feature selection -> 56 columns\n[    5.8s | 12388.8MB] Starting robust clean on features...\n[    6.3s | 12388.8MB] NaNs before fill: 0\n[    6.8s | 12388.8MB] NaNs after fill: 0\n[    7.2s | 12389.0MB] Final feature list length: 56\n[    7.2s | 12389.0MB] \n==============================\nTrain/Test Split\n==============================\n[    8.0s | 12235.0MB] X_train=(416458, 56), X_test=(138820, 56)\n[    8.0s | 12235.0MB] Train class distribution: [388186  28272] | Test: [129396   9424]\n[    8.1s | 12015.9MB] Normal-only count for AE: 388186\n[    8.9s | 11962.6MB] X_train_norm_s: shape=(388186, 56), dtype=float64, min=-2.1660, max=6.0000, mean=-0.0114, std=0.7254, NaNs=0\n[    9.0s | 11914.6MB] AE train subset: (349367, 56), AE val subset: (38819, 56)\n[    9.9s | 11857.4MB] X_train_sup_s: shape=(416458, 56), dtype=float64, min=-2.1401, max=608.6312, mean=0.0000, std=1.0000, NaNs=0\n[    9.9s | 11857.4MB] \n========================================\nCross-Validation Meta Ensemble\n========================================\n[   10.0s | 11857.4MB] \n==============================\nFold 1/3\n==============================\n[   10.0s | 11741.0MB] Fold train shape=(277638, 56), val shape=(138820, 56)\n[   10.0s | 11741.0MB] Fold class dist train=[258790  18848], val=[129396   9424]\n[   10.4s | 11638.0MB] Fold AE normal train=(232911, 56), normal val=(25879, 56)\n[   10.4s | 11638.0MB] Building AE (input_dim=56, latent_dim=32, width=128, dropout=0.25)\n[   10.5s | 11638.0MB] AE built successfully.\n[   10.8s | 11836.9MB] AE training started\n[   20.6s | 10909.7MB] Epoch 001/60 loss=0.190108 val_loss=0.027065 mae=0.254940 val_mae=0.072224\n[   23.0s | 10919.4MB] Epoch 002/60 loss=0.056872 val_loss=0.019890 mae=0.135410 val_mae=0.060600\n[   25.5s | 10922.9MB] Epoch 003/60 loss=0.050331 val_loss=0.016609 mae=0.129120 val_mae=0.056299\n[   27.9s | 10927.0MB] Epoch 004/60 loss=0.047071 val_loss=0.015171 mae=0.126256 val_mae=0.055659\n[   30.4s | 10929.0MB] Epoch 005/60 loss=0.045309 val_loss=0.014108 mae=0.124636 val_mae=0.053137\n[   32.8s | 10929.0MB] Epoch 006/60 loss=0.043801 val_loss=0.013367 mae=0.122837 val_mae=0.052507\n[   35.3s | 10930.2MB] Epoch 007/60 loss=0.042842 val_loss=0.012435 mae=0.121031 val_mae=0.051781\n[   37.8s | 10932.4MB] Epoch 008/60 loss=0.041953 val_loss=0.011482 mae=0.119918 val_mae=0.047975\n[   40.3s | 10935.1MB] Epoch 009/60 loss=0.041530 val_loss=0.011284 mae=0.119328 val_mae=0.047934\n[   42.7s | 10935.1MB] Epoch 010/60 loss=0.041133 val_loss=0.011234 mae=0.118781 val_mae=0.046363\n[   45.2s | 10935.6MB] Epoch 011/60 loss=0.040680 val_loss=0.010969 mae=0.118161 val_mae=0.047432\n[   47.6s | 10936.4MB] Epoch 012/60 loss=0.040337 val_loss=0.010622 mae=0.117601 val_mae=0.047174\n[   50.0s | 10943.7MB] Epoch 013/60 loss=0.040035 val_loss=0.010540 mae=0.117251 val_mae=0.044569\n[   52.4s | 10944.2MB] Epoch 014/60 loss=0.039796 val_loss=0.010431 mae=0.116938 val_mae=0.043938\n[   54.8s | 10946.0MB] Epoch 015/60 loss=0.039479 val_loss=0.009776 mae=0.116631 val_mae=0.043601\n[   57.2s | 10946.0MB] Epoch 016/60 loss=0.039251 val_loss=0.009915 mae=0.116322 val_mae=0.045527\n[   59.7s | 10950.6MB] Epoch 017/60 loss=0.039304 val_loss=0.010285 mae=0.116404 val_mae=0.044739\n[   62.1s | 10950.7MB] Epoch 018/60 loss=0.038961 val_loss=0.009852 mae=0.115879 val_mae=0.044551\n[   64.5s | 10951.0MB] Epoch 019/60 loss=0.038763 val_loss=0.009970 mae=0.115277 val_mae=0.043308\n[   67.0s | 10951.1MB] Epoch 020/60 loss=0.038057 val_loss=0.009146 mae=0.114039 val_mae=0.039135\n[   69.4s | 10956.2MB] Epoch 021/60 loss=0.037883 val_loss=0.009392 mae=0.113910 val_mae=0.040885\n[   71.9s | 10956.4MB] Epoch 022/60 loss=0.037643 val_loss=0.008449 mae=0.113647 val_mae=0.039248\n[   74.3s | 10956.5MB] Epoch 023/60 loss=0.037705 val_loss=0.008876 mae=0.113613 val_mae=0.039768\n[   76.8s | 10956.6MB] Epoch 024/60 loss=0.037640 val_loss=0.008700 mae=0.113562 val_mae=0.039217\n[   79.2s | 10956.7MB] Epoch 025/60 loss=0.037483 val_loss=0.008841 mae=0.113478 val_mae=0.039283\n[   81.6s | 10961.0MB] Epoch 026/60 loss=0.037633 val_loss=0.008858 mae=0.113504 val_mae=0.039738\n[   84.1s | 10961.2MB] Epoch 027/60 loss=0.037023 val_loss=0.008239 mae=0.112591 val_mae=0.036631\n[   86.5s | 10961.2MB] Epoch 028/60 loss=0.037003 val_loss=0.008223 mae=0.112545 val_mae=0.037120\n[   88.9s | 10961.2MB] Epoch 029/60 loss=0.036985 val_loss=0.008066 mae=0.112577 val_mae=0.035974\n[   91.4s | 10963.7MB] Epoch 030/60 loss=0.036971 val_loss=0.008133 mae=0.112578 val_mae=0.036070\n[   93.8s | 10963.7MB] Epoch 031/60 loss=0.036819 val_loss=0.008033 mae=0.112497 val_mae=0.036304\n[   96.3s | 10963.7MB] Epoch 032/60 loss=0.036913 val_loss=0.008084 mae=0.112483 val_mae=0.036938\n[   98.7s | 10963.9MB] Epoch 033/60 loss=0.036885 val_loss=0.008186 mae=0.112430 val_mae=0.036199\n[  101.1s | 10974.6MB] Epoch 034/60 loss=0.036740 val_loss=0.007978 mae=0.112105 val_mae=0.036400\n[  103.6s | 10974.6MB] Epoch 035/60 loss=0.036666 val_loss=0.007976 mae=0.112116 val_mae=0.036775\n[  106.1s | 10975.0MB] Epoch 036/60 loss=0.036733 val_loss=0.007914 mae=0.112122 val_mae=0.036562\n[  108.5s | 10975.0MB] Epoch 037/60 loss=0.036736 val_loss=0.007819 mae=0.112176 val_mae=0.036186\n[  110.9s | 10984.2MB] Epoch 038/60 loss=0.036762 val_loss=0.008023 mae=0.112099 val_mae=0.037403\n[  113.3s | 10985.2MB] Epoch 039/60 loss=0.036628 val_loss=0.007935 mae=0.112113 val_mae=0.036861\n[  115.8s | 10985.5MB] Epoch 040/60 loss=0.036639 val_loss=0.007926 mae=0.112084 val_mae=0.036450\n[  118.2s | 10985.5MB] Epoch 041/60 loss=0.036494 val_loss=0.007672 mae=0.111935 val_mae=0.035469\n[  120.6s | 10990.2MB] Epoch 042/60 loss=0.036422 val_loss=0.007651 mae=0.111856 val_mae=0.035262\n[  123.1s | 10990.2MB] Epoch 043/60 loss=0.036465 val_loss=0.007686 mae=0.111859 val_mae=0.036124\n[  125.5s | 10990.2MB] Epoch 044/60 loss=0.036368 val_loss=0.007591 mae=0.111787 val_mae=0.034605\n[  127.9s | 10990.4MB] Epoch 045/60 loss=0.036351 val_loss=0.007703 mae=0.111699 val_mae=0.035784\n[  130.3s | 10990.5MB] Epoch 046/60 loss=0.036419 val_loss=0.007570 mae=0.111719 val_mae=0.035919\n[  132.6s | 10992.4MB] Epoch 047/60 loss=0.036407 val_loss=0.007614 mae=0.111702 val_mae=0.035913\n[  135.0s | 10992.4MB] Epoch 048/60 loss=0.036410 val_loss=0.007630 mae=0.111811 val_mae=0.035893\n[  137.4s | 10992.4MB] Epoch 049/60 loss=0.036343 val_loss=0.007513 mae=0.111712 val_mae=0.035265\n[  139.9s | 10992.5MB] Epoch 050/60 loss=0.036323 val_loss=0.007640 mae=0.111645 val_mae=0.035869\n[  142.3s | 10996.4MB] Epoch 051/60 loss=0.036335 val_loss=0.007545 mae=0.111618 val_mae=0.035252\n[  144.8s | 10996.4MB] Epoch 052/60 loss=0.036240 val_loss=0.007523 mae=0.111669 val_mae=0.035152\n[  147.2s | 10996.4MB] Epoch 053/60 loss=0.036503 val_loss=0.007542 mae=0.111766 val_mae=0.035558\n[  149.6s | 10996.4MB] Epoch 054/60 loss=0.036384 val_loss=0.007529 mae=0.111646 val_mae=0.035340\n[  152.1s | 10996.9MB] Epoch 055/60 loss=0.036306 val_loss=0.007507 mae=0.111683 val_mae=0.035098\n[  154.5s | 10997.0MB] Epoch 056/60 loss=0.036271 val_loss=0.007513 mae=0.111562 val_mae=0.034889\n[  156.9s | 10997.0MB] Epoch 057/60 loss=0.036383 val_loss=0.007518 mae=0.111619 val_mae=0.035059\n[  159.3s | 10997.0MB] Epoch 058/60 loss=0.036154 val_loss=0.007489 mae=0.111511 val_mae=0.034867\n[  161.8s | 11000.4MB] Epoch 059/60 loss=0.036323 val_loss=0.007517 mae=0.111575 val_mae=0.035075\n[  164.2s | 11000.4MB] Epoch 060/60 loss=0.036312 val_loss=0.007510 mae=0.111612 val_mae=0.035021\n[  164.2s | 11000.4MB] AE training finished\n[  164.2s | 10900.7MB] AE fold training time: 153.7s\n[  173.1s | 11011.3MB] val_residuals: shape=(138820, 56), dtype=float64, min=-3.9213, max=4.7553, mean=-0.0016, std=0.1038, NaNs=0\n[  173.1s | 11011.3MB] recon_mse: shape=(138820,), dtype=float64, min=0.0002, max=1.7139, mean=0.0108, std=0.0370, NaNs=0\n[  188.8s | 11182.2MB] Computing Mahalanobis parameters for residuals shape=(258790, 56)\n[  188.9s | 11182.2MB] Mahalanobis prepared (cov regularized).\n[  189.7s | 11182.2MB] md_score_val: shape=(138820,), dtype=float64, min=1.0447, max=261.8400, mean=5.6649, std=6.6857, NaNs=0\n[  189.7s | 11182.2MB] Fitting GMM: residuals shape=(258790, 56), PCA_dim=12, components=3\n[  191.0s | 11184.4MB] PCA residuals shape: (258790, 12)\n[  209.2s | 11186.4MB] GMM fitted.\n[  209.3s | 11238.5MB] gmm_score_val: shape=(138820,), dtype=float64, min=-42.7961, max=256.0839, mean=-21.8044, std=16.7564, NaNs=0\n[  216.1s | 11269.7MB] X_val_lat: shape=(138820, 32), dtype=float32, min=0.0000, max=36.3236, mean=3.1517, std=4.2248, NaNs=0\n[  245.5s | 11368.4MB] lat_prob_val: shape=(138820,), dtype=float64, min=0.0000, max=1.0000, mean=0.0681, std=0.2428, NaNs=0\n[  260.6s | 11481.0MB] iso_score_val: shape=(138820,), dtype=float64, min=0.3312, max=0.7870, mean=0.3863, std=0.0588, NaNs=0\n[  260.6s | 11481.0MB] XGB pos_weight=13.7304\n[  268.0s | 11508.4MB] xgb_prob_val: shape=(138820,), dtype=float32, min=0.0000, max=1.0000, mean=0.0679, std=0.2515, NaNs=0\n[  268.0s | 11508.4MB] hybrid_val: shape=(138820,), dtype=float64, min=-0.4472, max=42.9709, mean=0.0000, std=0.9688, NaNs=0\n[  299.9s | 11519.9MB] Cluster 0 count=4288 hybrid_mean=0.6238 hybrid_std=1.9019\n[  299.9s | 11519.9MB] Cluster 1 count=61727 hybrid_mean=0.4614 hybrid_std=1.4836\n[  299.9s | 11519.9MB] Cluster 2 count=102305 hybrid_mean=-0.2627 hybrid_std=0.2115\n[  299.9s | 11519.9MB] Cluster 3 count=29872 hybrid_mean=-0.1506 hybrid_std=0.4187\n[  299.9s | 11519.9MB] Cluster 4 count=19811 hybrid_mean=-0.0245 hybrid_std=1.3274\n[  299.9s | 11519.9MB] Cluster 5 count=40787 hybrid_mean=0.0173 hybrid_std=0.7207\n[  299.9s | 11519.9MB] Fold 1 meta feature matrix shape: (138820, 5)\n[  299.9s | 11520.1MB] \n==============================\nFold 2/3\n==============================\n[  299.9s | 11579.5MB] Fold train shape=(277639, 56), val shape=(138819, 56)\n[  299.9s | 11579.5MB] Fold class dist train=[258791  18848], val=[129395   9424]\n[  300.3s | 11579.7MB] Fold AE normal train=(232911, 56), normal val=(25880, 56)\n[  300.3s | 11579.7MB] Building AE (input_dim=56, latent_dim=32, width=128, dropout=0.25)\n[  300.4s | 11579.7MB] AE built successfully.\n[  300.7s | 11778.7MB] AE training started\n[  310.3s | 11082.6MB] Epoch 001/60 loss=0.198453 val_loss=0.027713 mae=0.258604 val_mae=0.072612\n[  312.8s | 11099.5MB] Epoch 002/60 loss=0.060167 val_loss=0.020502 mae=0.141517 val_mae=0.065449\n[  315.2s | 11104.0MB] Epoch 003/60 loss=0.053787 val_loss=0.017630 mae=0.134669 val_mae=0.062338\n[  317.7s | 11107.5MB] Epoch 004/60 loss=0.050156 val_loss=0.015540 mae=0.130725 val_mae=0.057913\n[  320.1s | 11109.5MB] Epoch 005/60 loss=0.048194 val_loss=0.014382 mae=0.128504 val_mae=0.056056\n[  322.5s | 11112.0MB] Epoch 006/60 loss=0.046728 val_loss=0.013360 mae=0.126543 val_mae=0.054456\n[  325.0s | 11116.1MB] Epoch 007/60 loss=0.045656 val_loss=0.012994 mae=0.124844 val_mae=0.052476\n[  327.4s | 11117.9MB] Epoch 008/60 loss=0.044981 val_loss=0.012205 mae=0.123885 val_mae=0.051149\n[  329.9s | 11118.6MB] Epoch 009/60 loss=0.044114 val_loss=0.011747 mae=0.122919 val_mae=0.052630\n[  332.3s | 11119.2MB] Epoch 010/60 loss=0.043609 val_loss=0.011113 mae=0.122477 val_mae=0.050460\n[  334.7s | 11120.4MB] Epoch 011/60 loss=0.043129 val_loss=0.010546 mae=0.122062 val_mae=0.047727\n[  337.2s | 11122.4MB] Epoch 012/60 loss=0.042797 val_loss=0.010797 mae=0.121499 val_mae=0.049609\n[  339.6s | 11122.5MB] Epoch 013/60 loss=0.042358 val_loss=0.010451 mae=0.120912 val_mae=0.048874\n[  342.0s | 11122.5MB] Epoch 014/60 loss=0.042142 val_loss=0.011003 mae=0.120685 val_mae=0.051591\n[  344.5s | 11124.5MB] Epoch 015/60 loss=0.041939 val_loss=0.010447 mae=0.120300 val_mae=0.049540\n[  346.9s | 11124.7MB] Epoch 016/60 loss=0.040875 val_loss=0.009497 mae=0.118802 val_mae=0.043507\n[  349.4s | 11125.4MB] Epoch 017/60 loss=0.040728 val_loss=0.009064 mae=0.118645 val_mae=0.041612\n[  351.9s | 11125.4MB] Epoch 018/60 loss=0.040761 val_loss=0.009260 mae=0.118500 val_mae=0.043547\n[  354.3s | 11126.9MB] Epoch 019/60 loss=0.040639 val_loss=0.009221 mae=0.118468 val_mae=0.044597\n[  356.7s | 11127.0MB] Epoch 020/60 loss=0.040713 val_loss=0.008872 mae=0.118466 val_mae=0.042188\n[  359.2s | 11127.0MB] Epoch 021/60 loss=0.040467 val_loss=0.009021 mae=0.118262 val_mae=0.043487\n[  361.6s | 11127.4MB] Epoch 022/60 loss=0.040358 val_loss=0.009094 mae=0.118060 val_mae=0.041956\n[  364.1s | 11127.4MB] Epoch 023/60 loss=0.040267 val_loss=0.009138 mae=0.117903 val_mae=0.042252\n[  366.5s | 11127.4MB] Epoch 024/60 loss=0.040095 val_loss=0.008766 mae=0.117604 val_mae=0.041101\n[  368.9s | 11127.2MB] Epoch 025/60 loss=0.040025 val_loss=0.009116 mae=0.117557 val_mae=0.042788\n[  371.3s | 11127.4MB] Epoch 026/60 loss=0.040113 val_loss=0.008713 mae=0.117731 val_mae=0.042083\n[  373.6s | 11127.5MB] Epoch 027/60 loss=0.039990 val_loss=0.008797 mae=0.117515 val_mae=0.042315\n[  376.0s | 11127.5MB] Epoch 028/60 loss=0.039872 val_loss=0.008905 mae=0.117291 val_mae=0.041807\n[  378.4s | 11128.9MB] Epoch 029/60 loss=0.039378 val_loss=0.008230 mae=0.116682 val_mae=0.038808\n[  380.9s | 11129.0MB] Epoch 030/60 loss=0.039468 val_loss=0.008163 mae=0.116573 val_mae=0.039715\n[  383.3s | 11129.0MB] Epoch 031/60 loss=0.039393 val_loss=0.008189 mae=0.116545 val_mae=0.038367\n[  385.8s | 11129.0MB] Epoch 032/60 loss=0.039284 val_loss=0.008042 mae=0.116320 val_mae=0.038155\n[  388.2s | 11134.9MB] Epoch 033/60 loss=0.039120 val_loss=0.008208 mae=0.116070 val_mae=0.038698\n[  390.6s | 11135.0MB] Epoch 034/60 loss=0.039265 val_loss=0.008031 mae=0.116159 val_mae=0.039036\n[  393.1s | 11135.1MB] Epoch 035/60 loss=0.039083 val_loss=0.007944 mae=0.116006 val_mae=0.038873\n[  395.5s | 11135.1MB] Epoch 036/60 loss=0.039103 val_loss=0.008304 mae=0.115852 val_mae=0.039805\n[  398.0s | 11137.5MB] Epoch 037/60 loss=0.039047 val_loss=0.007975 mae=0.115753 val_mae=0.036761\n[  400.4s | 11137.5MB] Epoch 038/60 loss=0.038832 val_loss=0.007888 mae=0.115550 val_mae=0.036334\n[  402.8s | 11137.7MB] Epoch 039/60 loss=0.038856 val_loss=0.007826 mae=0.115534 val_mae=0.036238\n[  405.2s | 11137.6MB] Epoch 040/60 loss=0.038750 val_loss=0.007924 mae=0.115442 val_mae=0.037131\n[  407.7s | 11140.7MB] Epoch 041/60 loss=0.038868 val_loss=0.007836 mae=0.115375 val_mae=0.036928\n[  410.1s | 11141.0MB] Epoch 042/60 loss=0.038802 val_loss=0.007879 mae=0.115394 val_mae=0.036786\n[  412.5s | 11141.1MB] Epoch 043/60 loss=0.038559 val_loss=0.007742 mae=0.115130 val_mae=0.035598\n[  414.9s | 11141.1MB] Epoch 044/60 loss=0.038587 val_loss=0.007862 mae=0.115061 val_mae=0.036506\n[  417.4s | 11141.0MB] Epoch 045/60 loss=0.038588 val_loss=0.007676 mae=0.115012 val_mae=0.035425\n[  419.8s | 11144.6MB] Epoch 046/60 loss=0.038685 val_loss=0.007793 mae=0.115103 val_mae=0.036178\n[  422.2s | 11144.7MB] Epoch 047/60 loss=0.038552 val_loss=0.007724 mae=0.114843 val_mae=0.035591\n[  424.7s | 11144.7MB] Epoch 048/60 loss=0.038585 val_loss=0.007684 mae=0.114989 val_mae=0.035410\n[  427.1s | 11144.7MB] Epoch 049/60 loss=0.038540 val_loss=0.007704 mae=0.114907 val_mae=0.035411\n[  429.5s | 11144.8MB] Epoch 050/60 loss=0.038628 val_loss=0.007698 mae=0.114897 val_mae=0.035262\n[  432.0s | 11144.8MB] Epoch 051/60 loss=0.038372 val_loss=0.007730 mae=0.114794 val_mae=0.035278\n[  434.4s | 11144.8MB] Epoch 052/60 loss=0.038496 val_loss=0.007697 mae=0.114912 val_mae=0.035361\n[  436.8s | 11144.8MB] Epoch 053/60 loss=0.038473 val_loss=0.007651 mae=0.114807 val_mae=0.035116\n[  439.3s | 11149.7MB] Epoch 054/60 loss=0.038578 val_loss=0.007663 mae=0.114830 val_mae=0.035156\n[  441.8s | 11149.7MB] Epoch 055/60 loss=0.038496 val_loss=0.007721 mae=0.114801 val_mae=0.035237\n[  444.2s | 11149.7MB] Epoch 056/60 loss=0.038553 val_loss=0.007700 mae=0.114815 val_mae=0.035395\n[  446.6s | 11149.7MB] Epoch 057/60 loss=0.038603 val_loss=0.007691 mae=0.114855 val_mae=0.035273\n[  449.0s | 11152.8MB] Epoch 058/60 loss=0.038213 val_loss=0.007717 mae=0.114679 val_mae=0.035391\n[  451.4s | 11152.8MB] Epoch 059/60 loss=0.038538 val_loss=0.007662 mae=0.114738 val_mae=0.035286\n[  453.8s | 11152.8MB] Epoch 060/60 loss=0.038549 val_loss=0.007663 mae=0.114797 val_mae=0.035266\n[  453.8s | 11152.8MB] AE training finished\n[  453.8s | 11053.1MB] AE fold training time: 153.5s\n[  461.7s | 11260.2MB] val_residuals: shape=(138819, 56), dtype=float64, min=-4.5259, max=6.0019, mean=0.0029, std=0.1063, NaNs=0\n[  461.7s | 11260.2MB] recon_mse: shape=(138819,), dtype=float64, min=0.0002, max=2.0872, mean=0.0113, std=0.0378, NaNs=0\n[  475.5s | 11291.1MB] Computing Mahalanobis parameters for residuals shape=(258791, 56)\n[  475.6s | 11291.1MB] Mahalanobis prepared (cov regularized).\n[  476.4s | 11291.1MB] md_score_val: shape=(138819,), dtype=float64, min=1.1099, max=513.4273, mean=5.7111, std=7.2348, NaNs=0\n[  476.4s | 11291.1MB] Fitting GMM: residuals shape=(258791, 56), PCA_dim=12, components=3\n[  477.7s | 11297.8MB] PCA residuals shape: (258791, 12)\n[  485.3s | 11304.5MB] GMM fitted.\n[  485.4s | 11364.1MB] gmm_score_val: shape=(138819,), dtype=float64, min=-41.0306, max=183.9668, mean=-21.9371, std=15.6787, NaNs=0\n[  492.4s | 11391.3MB] X_val_lat: shape=(138819, 32), dtype=float32, min=0.0000, max=27.0368, mean=2.5013, std=3.4214, NaNs=0\n[  518.6s | 11433.8MB] lat_prob_val: shape=(138819,), dtype=float64, min=0.0000, max=1.0000, mean=0.0673, std=0.2393, NaNs=0\n[  533.6s | 11521.3MB] iso_score_val: shape=(138819,), dtype=float64, min=0.3332, max=0.7800, mean=0.3865, std=0.0581, NaNs=0\n[  533.6s | 11521.3MB] XGB pos_weight=13.7304\n[  541.1s | 11482.1MB] xgb_prob_val: shape=(138819,), dtype=float32, min=0.0000, max=1.0000, mean=0.0679, std=0.2516, NaNs=0\n[  541.1s | 11482.1MB] hybrid_val: shape=(138819,), dtype=float64, min=-0.4303, max=49.1811, mean=-0.0000, std=0.9639, NaNs=0\n[  572.2s | 11493.7MB] Cluster 0 count=102515 hybrid_mean=-0.2717 hybrid_std=0.2435\n[  572.2s | 11493.7MB] Cluster 1 count=19184 hybrid_mean=-0.1940 hybrid_std=0.3044\n[  572.2s | 11493.7MB] Cluster 2 count=29518 hybrid_mean=-0.1883 hybrid_std=0.2679\n[  572.2s | 11493.7MB] Cluster 3 count=61940 hybrid_mean=0.5345 hybrid_std=1.5980\n[  572.2s | 11493.7MB] Cluster 4 count=28116 hybrid_mean=0.0221 hybrid_std=1.0161\n[  572.2s | 11493.7MB] Cluster 5 count=17518 hybrid_mean=0.1943 hybrid_std=0.9364\n[  572.2s | 11494.1MB] Fold 2 meta feature matrix shape: (138819, 5)\n[  572.2s | 11494.1MB] \n==============================\nFold 3/3\n==============================\n[  572.2s | 11494.2MB] Fold train shape=(277639, 56), val shape=(138819, 56)\n[  572.2s | 11494.2MB] Fold class dist train=[258791  18848], val=[129395   9424]\n[  572.6s | 11499.0MB] Fold AE normal train=(232911, 56), normal val=(25880, 56)\n[  572.6s | 11499.0MB] Building AE (input_dim=56, latent_dim=32, width=128, dropout=0.25)\n[  572.7s | 11499.0MB] AE built successfully.\n[  573.0s | 11698.1MB] AE training started\n[  582.5s | 11223.0MB] Epoch 001/60 loss=0.198346 val_loss=0.026937 mae=0.257155 val_mae=0.071256\n[  585.0s | 11233.6MB] Epoch 002/60 loss=0.057763 val_loss=0.020104 mae=0.136072 val_mae=0.061208\n[  587.5s | 11235.4MB] Epoch 003/60 loss=0.050693 val_loss=0.016889 mae=0.129420 val_mae=0.056757\n[  590.1s | 11235.9MB] Epoch 004/60 loss=0.047367 val_loss=0.015171 mae=0.125933 val_mae=0.053137\n[  592.6s | 11237.5MB] Epoch 005/60 loss=0.045083 val_loss=0.013152 mae=0.123457 val_mae=0.048774\n[  595.1s | 11238.9MB] Epoch 006/60 loss=0.043667 val_loss=0.012429 mae=0.121645 val_mae=0.048179\n[  597.6s | 11241.0MB] Epoch 007/60 loss=0.042848 val_loss=0.012344 mae=0.120515 val_mae=0.051217\n[  600.1s | 11244.6MB] Epoch 008/60 loss=0.042033 val_loss=0.011502 mae=0.119496 val_mae=0.046496\n[  602.6s | 11246.4MB] Epoch 009/60 loss=0.041512 val_loss=0.011408 mae=0.118898 val_mae=0.048156\n[  605.0s | 11246.9MB] Epoch 010/60 loss=0.040923 val_loss=0.010899 mae=0.117716 val_mae=0.044085\n[  607.5s | 11247.2MB] Epoch 011/60 loss=0.040466 val_loss=0.010672 mae=0.117000 val_mae=0.045553\n[  609.9s | 11247.4MB] Epoch 012/60 loss=0.039946 val_loss=0.009946 mae=0.116576 val_mae=0.042858\n[  612.3s | 11249.6MB] Epoch 013/60 loss=0.039688 val_loss=0.009704 mae=0.116385 val_mae=0.042696\n[  614.7s | 11249.7MB] Epoch 014/60 loss=0.039368 val_loss=0.009732 mae=0.115926 val_mae=0.045700\n[  617.1s | 11250.0MB] Epoch 015/60 loss=0.039069 val_loss=0.009321 mae=0.115595 val_mae=0.043267\n[  619.5s | 11250.0MB] Epoch 016/60 loss=0.038905 val_loss=0.008934 mae=0.115277 val_mae=0.041280\n[  622.0s | 11250.7MB] Epoch 017/60 loss=0.038656 val_loss=0.008985 mae=0.115093 val_mae=0.040979\n[  624.4s | 11251.0MB] Epoch 018/60 loss=0.038484 val_loss=0.008846 mae=0.114795 val_mae=0.040888\n[  626.9s | 11251.0MB] Epoch 019/60 loss=0.038372 val_loss=0.009053 mae=0.114660 val_mae=0.043561\n[  629.4s | 11251.5MB] Epoch 020/60 loss=0.038207 val_loss=0.008746 mae=0.114514 val_mae=0.040839\n[  631.8s | 11251.5MB] Epoch 021/60 loss=0.037907 val_loss=0.008638 mae=0.114165 val_mae=0.038863\n[  634.2s | 11251.6MB] Epoch 022/60 loss=0.037839 val_loss=0.008507 mae=0.114015 val_mae=0.040011\n[  636.7s | 11251.6MB] Epoch 023/60 loss=0.037816 val_loss=0.008226 mae=0.114007 val_mae=0.038929\n[  639.2s | 11251.7MB] Epoch 024/60 loss=0.037676 val_loss=0.007923 mae=0.113879 val_mae=0.038906\n[  641.6s | 11251.7MB] Epoch 025/60 loss=0.037682 val_loss=0.008454 mae=0.113749 val_mae=0.040418\n[  644.0s | 11251.7MB] Epoch 026/60 loss=0.037427 val_loss=0.008415 mae=0.113605 val_mae=0.039510\n[  646.5s | 11252.1MB] Epoch 027/60 loss=0.037413 val_loss=0.007864 mae=0.113544 val_mae=0.038244\n[  649.0s | 11252.2MB] Epoch 028/60 loss=0.037346 val_loss=0.008650 mae=0.113386 val_mae=0.040995\n[  651.4s | 11254.1MB] Epoch 029/60 loss=0.036723 val_loss=0.007772 mae=0.112351 val_mae=0.036054\n[  653.8s | 11254.2MB] Epoch 030/60 loss=0.036416 val_loss=0.007436 mae=0.112172 val_mae=0.035466\n[  656.3s | 11254.5MB] Epoch 031/60 loss=0.036565 val_loss=0.007393 mae=0.112256 val_mae=0.036610\n[  658.7s | 11254.6MB] Epoch 032/60 loss=0.036406 val_loss=0.007308 mae=0.112082 val_mae=0.035943\n[  661.2s | 11254.7MB] Epoch 033/60 loss=0.036305 val_loss=0.007420 mae=0.112010 val_mae=0.035316\n[  663.6s | 11254.9MB] Epoch 034/60 loss=0.036381 val_loss=0.007314 mae=0.112065 val_mae=0.035068\n[  666.1s | 11254.9MB] Epoch 035/60 loss=0.036196 val_loss=0.007376 mae=0.111950 val_mae=0.035924\n[  668.5s | 11254.9MB] Epoch 036/60 loss=0.036189 val_loss=0.007353 mae=0.111965 val_mae=0.036891\n[  671.0s | 11255.0MB] Epoch 037/60 loss=0.035938 val_loss=0.006912 mae=0.111343 val_mae=0.033351\n[  673.4s | 11255.0MB] Epoch 038/60 loss=0.036022 val_loss=0.006954 mae=0.111489 val_mae=0.032882\n[  675.9s | 11255.1MB] Epoch 039/60 loss=0.035812 val_loss=0.007183 mae=0.111351 val_mae=0.035045\n[  678.3s | 11255.1MB] Epoch 040/60 loss=0.035786 val_loss=0.007119 mae=0.111274 val_mae=0.034202\n[  680.8s | 11255.1MB] Epoch 041/60 loss=0.035905 val_loss=0.007237 mae=0.111304 val_mae=0.035479\n[  683.3s | 11255.1MB] Epoch 042/60 loss=0.035784 val_loss=0.006804 mae=0.111030 val_mae=0.033116\n[  685.8s | 11255.1MB] Epoch 043/60 loss=0.035727 val_loss=0.006842 mae=0.111054 val_mae=0.033150\n[  688.2s | 11255.1MB] Epoch 044/60 loss=0.035723 val_loss=0.006888 mae=0.111025 val_mae=0.033121\n[  690.6s | 11255.2MB] Epoch 045/60 loss=0.035624 val_loss=0.006807 mae=0.110920 val_mae=0.032905\n[  693.1s | 11255.2MB] Epoch 046/60 loss=0.035530 val_loss=0.006874 mae=0.110880 val_mae=0.033287\n[  695.4s | 11255.4MB] Epoch 047/60 loss=0.035563 val_loss=0.006744 mae=0.110768 val_mae=0.032765\n[  697.9s | 11255.4MB] Epoch 048/60 loss=0.035630 val_loss=0.006755 mae=0.110969 val_mae=0.033175\n[  700.2s | 11255.5MB] Epoch 049/60 loss=0.035448 val_loss=0.006767 mae=0.110842 val_mae=0.033201\n[  702.6s | 11255.3MB] Epoch 050/60 loss=0.035440 val_loss=0.006745 mae=0.110775 val_mae=0.033106\n[  705.0s | 11255.5MB] Epoch 051/60 loss=0.035500 val_loss=0.006750 mae=0.110783 val_mae=0.033213\n[  707.5s | 11255.5MB] Epoch 052/60 loss=0.035465 val_loss=0.006741 mae=0.110738 val_mae=0.032847\n[  710.0s | 11255.6MB] Epoch 053/60 loss=0.035541 val_loss=0.006693 mae=0.110860 val_mae=0.032863\n[  712.4s | 11255.8MB] Epoch 054/60 loss=0.035548 val_loss=0.006672 mae=0.110843 val_mae=0.032576\n[  714.8s | 11257.3MB] Epoch 055/60 loss=0.035447 val_loss=0.006686 mae=0.110752 val_mae=0.032664\n[  717.3s | 11258.6MB] Epoch 056/60 loss=0.035343 val_loss=0.006691 mae=0.110695 val_mae=0.032726\n[  719.7s | 11258.6MB] Epoch 057/60 loss=0.035415 val_loss=0.006747 mae=0.110683 val_mae=0.032909\n[  722.1s | 11258.7MB] Epoch 058/60 loss=0.035433 val_loss=0.006740 mae=0.110616 val_mae=0.032896\n[  724.6s | 11258.7MB] Epoch 059/60 loss=0.035362 val_loss=0.006690 mae=0.110691 val_mae=0.032767\n[  727.0s | 11262.3MB] Epoch 060/60 loss=0.035421 val_loss=0.006717 mae=0.110705 val_mae=0.033133\n[  727.0s | 11262.3MB] AE training finished\n[  727.0s | 11162.8MB] AE fold training time: 154.4s\n[  734.9s | 11364.8MB] val_residuals: shape=(138819, 56), dtype=float64, min=-3.7647, max=6.0017, mean=0.0008, std=0.1011, NaNs=0\n[  734.9s | 11364.8MB] recon_mse: shape=(138819,), dtype=float64, min=0.0002, max=1.7681, mean=0.0102, std=0.0353, NaNs=0\n[  748.7s | 11400.6MB] Computing Mahalanobis parameters for residuals shape=(258791, 56)\n[  748.8s | 11400.6MB] Mahalanobis prepared (cov regularized).\n[  749.7s | 11400.6MB] md_score_val: shape=(138819,), dtype=float64, min=1.0782, max=255.3766, mean=6.0286, std=7.6749, NaNs=0\n[  749.7s | 11400.6MB] Fitting GMM: residuals shape=(258791, 56), PCA_dim=12, components=3\n[  751.0s | 11401.5MB] PCA residuals shape: (258791, 12)\n[  755.4s | 11404.4MB] GMM fitted.\n[  755.6s | 11457.3MB] gmm_score_val: shape=(138819,), dtype=float64, min=-33.7558, max=515.1350, mean=-20.1817, std=15.9324, NaNs=0\n[  762.6s | 11504.1MB] X_val_lat: shape=(138819, 32), dtype=float32, min=0.0000, max=36.8185, mean=2.9408, std=4.2837, NaNs=0\n[  792.0s | 11538.2MB] lat_prob_val: shape=(138819,), dtype=float64, min=0.0000, max=1.0000, mean=0.0681, std=0.2296, NaNs=0\n[  807.0s | 11594.2MB] iso_score_val: shape=(138819,), dtype=float64, min=0.3312, max=0.7780, mean=0.3869, std=0.0584, NaNs=0\n[  807.0s | 11594.2MB] XGB pos_weight=13.7304\n[  814.4s | 11661.2MB] xgb_prob_val: shape=(138819,), dtype=float32, min=0.0000, max=1.0000, mean=0.0679, std=0.2515, NaNs=0\n[  814.4s | 11661.2MB] hybrid_val: shape=(138819,), dtype=float64, min=-0.4288, max=36.9540, mean=-0.0000, std=0.9661, NaNs=0\n[  845.4s | 11628.0MB] Cluster 0 count=40852 hybrid_mean=0.0431 hybrid_std=0.8373\n[  845.4s | 11628.0MB] Cluster 1 count=17725 hybrid_mean=-0.1891 hybrid_std=0.5252\n[  845.4s | 11628.0MB] Cluster 2 count=29873 hybrid_mean=-0.1312 hybrid_std=0.8196\n[  845.4s | 11628.0MB] Cluster 3 count=63580 hybrid_mean=0.4671 hybrid_std=1.5255\n[  845.4s | 11628.0MB] Cluster 4 count=102385 hybrid_mean=-0.2612 hybrid_std=0.2171\n[  845.4s | 11628.0MB] Cluster 5 count=4376 hybrid_mean=0.5842 hybrid_std=1.5785\n[  845.4s | 11628.0MB] Fold 3 meta feature matrix shape: (138819, 5)\n[  845.4s | 11628.0MB] \n================================\nMeta Ensemble Training\n================================\n[  845.4s | 11628.0MB] oof_X: shape=(416458, 5), dtype=float64, min=-42.7961, max=515.1350, mean=-4.1571, std=11.2215, NaNs=0\n[  845.4s | 11628.0MB] oof_y distribution: [388186  28272]\n[  847.6s | 11634.3MB] oof_meta_prob: shape=(416458,), dtype=float64, min=0.0000, max=1.0000, mean=0.0680, std=0.2515, NaNs=0\n[  847.6s | 11634.3MB] Computing PR-based threshold...\n[  847.7s | 11636.3MB] Target-precision threshold selected: {'kind': 'target_precision_0.95', 'threshold': 0.0014975574012455514, 'precision': 0.9500302439680086, 'recall': 0.9999646293152236, 'f1': 0.9743580906422713}\n[  847.7s | 11636.3MB] Meta threshold selected: {'kind': 'target_precision_0.95', 'threshold': 0.0014975574012455514, 'precision': 0.9500302439680086, 'recall': 0.9999646293152236, 'f1': 0.9743580906422713}\n[  847.7s | 11636.3MB] \n=========================================\nFinal Model Fit (Full Training)\n=========================================\n[  848.6s | 11603.8MB] X_train_norm_s_full: shape=(388186, 56), dtype=float64, min=-2.1660, max=6.0000, mean=-0.0114, std=0.7254, NaNs=0\n[  848.7s | 11555.8MB] Building AE (input_dim=56, latent_dim=32, width=128, dropout=0.25)\n[  848.8s | 11555.8MB] AE built successfully.\n[  849.3s | 11262.0MB] AE training started\n[  859.9s | 11290.1MB] Epoch 001/60 loss=0.150318 val_loss=0.022692 mae=0.216814 val_mae=0.067114\n[  863.4s | 11298.8MB] Epoch 002/60 loss=0.052154 val_loss=0.016474 mae=0.130447 val_mae=0.057819\n[  867.0s | 11303.3MB] Epoch 003/60 loss=0.046723 val_loss=0.013858 mae=0.124331 val_mae=0.053393\n[  870.6s | 11311.5MB] Epoch 004/60 loss=0.044063 val_loss=0.012533 mae=0.121089 val_mae=0.049188\n[  874.2s | 11311.5MB] Epoch 005/60 loss=0.042698 val_loss=0.011640 mae=0.119665 val_mae=0.048853\n[  877.9s | 11316.6MB] Epoch 006/60 loss=0.041840 val_loss=0.011433 mae=0.118838 val_mae=0.048882\n[  881.4s | 11321.2MB] Epoch 007/60 loss=0.040930 val_loss=0.009952 mae=0.118102 val_mae=0.044532\n[  885.0s | 11322.5MB] Epoch 008/60 loss=0.040418 val_loss=0.009955 mae=0.117443 val_mae=0.045750\n[  888.6s | 11327.3MB] Epoch 009/60 loss=0.039916 val_loss=0.009699 mae=0.117049 val_mae=0.045012\n[  892.2s | 11329.0MB] Epoch 010/60 loss=0.039577 val_loss=0.009351 mae=0.116377 val_mae=0.042665\n[  895.8s | 11330.7MB] Epoch 011/60 loss=0.039054 val_loss=0.008993 mae=0.115269 val_mae=0.042023\n[  899.5s | 11333.0MB] Epoch 012/60 loss=0.038905 val_loss=0.008797 mae=0.115034 val_mae=0.041872\n[  903.2s | 11333.7MB] Epoch 013/60 loss=0.038741 val_loss=0.009136 mae=0.114896 val_mae=0.044519\n[  907.0s | 11333.7MB] Epoch 014/60 loss=0.038344 val_loss=0.008823 mae=0.114471 val_mae=0.042962\n[  910.6s | 11335.5MB] Epoch 015/60 loss=0.038277 val_loss=0.008486 mae=0.114461 val_mae=0.039887\n[  914.2s | 11336.2MB] Epoch 016/60 loss=0.037994 val_loss=0.008247 mae=0.114100 val_mae=0.039915\n[  917.9s | 11336.3MB] Epoch 017/60 loss=0.037880 val_loss=0.008458 mae=0.113944 val_mae=0.040932\n[  921.4s | 11336.5MB] Epoch 018/60 loss=0.037834 val_loss=0.008140 mae=0.113794 val_mae=0.037966\n[  925.0s | 11336.5MB] Epoch 019/60 loss=0.037684 val_loss=0.007946 mae=0.113614 val_mae=0.038699\n[  928.6s | 11336.5MB] Epoch 020/60 loss=0.037380 val_loss=0.008294 mae=0.113294 val_mae=0.041407\n[  932.2s | 11336.8MB] Epoch 021/60 loss=0.037233 val_loss=0.008562 mae=0.112989 val_mae=0.042535\n[  935.8s | 11337.8MB] Epoch 022/60 loss=0.037315 val_loss=0.007891 mae=0.113071 val_mae=0.039456\n[  939.3s | 11338.1MB] Epoch 023/60 loss=0.037293 val_loss=0.008208 mae=0.113164 val_mae=0.042722\n[  942.8s | 11338.2MB] Epoch 024/60 loss=0.036522 val_loss=0.007498 mae=0.111963 val_mae=0.036813\n[  946.3s | 11338.5MB] Epoch 025/60 loss=0.036438 val_loss=0.007516 mae=0.111710 val_mae=0.037406\n[  949.9s | 11338.5MB] Epoch 026/60 loss=0.036295 val_loss=0.007412 mae=0.111669 val_mae=0.037150\n[  953.5s | 11338.5MB] Epoch 027/60 loss=0.036286 val_loss=0.007347 mae=0.111579 val_mae=0.036203\n[  957.1s | 11338.6MB] Epoch 028/60 loss=0.036195 val_loss=0.007401 mae=0.111475 val_mae=0.037864\n[  960.7s | 11338.6MB] Epoch 029/60 loss=0.036231 val_loss=0.007368 mae=0.111386 val_mae=0.036484\n[  964.2s | 11341.8MB] Epoch 030/60 loss=0.036323 val_loss=0.007355 mae=0.111405 val_mae=0.036361\n[  967.8s | 11341.8MB] Epoch 031/60 loss=0.036060 val_loss=0.007266 mae=0.111212 val_mae=0.036403\n[  971.4s | 11343.4MB] Epoch 032/60 loss=0.035665 val_loss=0.007193 mae=0.110643 val_mae=0.034880\n[  975.0s | 11343.7MB] Epoch 033/60 loss=0.035777 val_loss=0.007177 mae=0.110716 val_mae=0.035130\n[  978.6s | 11343.7MB] Epoch 034/60 loss=0.035658 val_loss=0.007367 mae=0.110657 val_mae=0.035990\n[  982.2s | 11343.8MB] Epoch 035/60 loss=0.035759 val_loss=0.007187 mae=0.110768 val_mae=0.035394\n[  985.7s | 11343.8MB] Epoch 036/60 loss=0.035726 val_loss=0.007066 mae=0.110618 val_mae=0.034621\n[  989.3s | 11343.8MB] Epoch 037/60 loss=0.035694 val_loss=0.007054 mae=0.110611 val_mae=0.034779\n[  992.9s | 11343.8MB] Epoch 038/60 loss=0.035637 val_loss=0.006942 mae=0.110414 val_mae=0.033960\n[  996.5s | 11343.8MB] Epoch 039/60 loss=0.035579 val_loss=0.006988 mae=0.110356 val_mae=0.034297\n[ 1000.1s | 11346.5MB] Epoch 040/60 loss=0.035555 val_loss=0.007056 mae=0.110249 val_mae=0.035451\n[ 1003.6s | 11347.9MB] Epoch 041/60 loss=0.035607 val_loss=0.007088 mae=0.110294 val_mae=0.035829\n[ 1007.3s | 11348.0MB] Epoch 042/60 loss=0.035609 val_loss=0.007059 mae=0.110361 val_mae=0.035779\n[ 1010.9s | 11348.0MB] Epoch 043/60 loss=0.035395 val_loss=0.006743 mae=0.110048 val_mae=0.032894\n[ 1014.5s | 11349.2MB] Epoch 044/60 loss=0.035380 val_loss=0.006816 mae=0.110043 val_mae=0.033080\n[ 1018.0s | 11349.2MB] Epoch 045/60 loss=0.035324 val_loss=0.006940 mae=0.109951 val_mae=0.034035\n[ 1021.5s | 11349.2MB] Epoch 046/60 loss=0.035359 val_loss=0.006819 mae=0.110029 val_mae=0.033259\n[ 1025.0s | 11351.3MB] Epoch 047/60 loss=0.035459 val_loss=0.006758 mae=0.110094 val_mae=0.033033\n[ 1028.5s | 11351.4MB] Epoch 048/60 loss=0.035285 val_loss=0.006745 mae=0.109895 val_mae=0.032926\n[ 1032.1s | 11354.3MB] Epoch 049/60 loss=0.035130 val_loss=0.006715 mae=0.109797 val_mae=0.032734\n[ 1035.6s | 11354.4MB] Epoch 050/60 loss=0.035297 val_loss=0.006761 mae=0.109898 val_mae=0.033158\n[ 1039.2s | 11354.4MB] Epoch 051/60 loss=0.035280 val_loss=0.006679 mae=0.109830 val_mae=0.032486\n[ 1042.8s | 11356.9MB] Epoch 052/60 loss=0.035274 val_loss=0.006681 mae=0.109882 val_mae=0.032589\n[ 1046.4s | 11356.9MB] Epoch 053/60 loss=0.035238 val_loss=0.006689 mae=0.109798 val_mae=0.032610\n[ 1050.0s | 11357.0MB] Epoch 054/60 loss=0.035223 val_loss=0.006739 mae=0.109860 val_mae=0.033007\n[ 1053.6s | 11357.2MB] Epoch 055/60 loss=0.035342 val_loss=0.006704 mae=0.109924 val_mae=0.032561\n[ 1057.1s | 11357.3MB] Epoch 056/60 loss=0.035216 val_loss=0.006667 mae=0.109830 val_mae=0.032659\n[ 1060.7s | 11357.4MB] Epoch 057/60 loss=0.035134 val_loss=0.006679 mae=0.109725 val_mae=0.032646\n[ 1064.3s | 11358.3MB] Epoch 058/60 loss=0.035220 val_loss=0.006636 mae=0.109753 val_mae=0.032473\n[ 1067.9s | 11358.7MB] Epoch 059/60 loss=0.035080 val_loss=0.006697 mae=0.109661 val_mae=0.032506\n[ 1071.5s | 11358.8MB] Epoch 060/60 loss=0.035155 val_loss=0.006646 mae=0.109747 val_mae=0.032507\n[ 1071.5s | 11358.8MB] AE training finished\n[ 1096.1s | 11314.5MB] Computing Mahalanobis parameters for residuals shape=(388186, 56)\n[ 1096.3s | 11314.5MB] Mahalanobis prepared (cov regularized).\n[ 1096.3s | 11314.5MB] Fitting GMM: residuals shape=(388186, 56), PCA_dim=12, components=3\n[ 1098.2s | 11392.5MB] PCA residuals shape: (388186, 12)\n[ 1117.6s | 11527.0MB] GMM fitted.\n[ 1143.9s | 11520.9MB] Final cluster 0 threshold=1.069914 count=44054\n[ 1144.0s | 11520.9MB] Final cluster 1 threshold=0.845202 count=153849\n[ 1144.0s | 11520.9MB] Final cluster 2 threshold=11.290761 count=93968\n[ 1144.0s | 11520.9MB] Final cluster 3 threshold=1.335828 count=26982\n[ 1144.0s | 11520.9MB] Final cluster 4 threshold=5.158322 count=24619\n[ 1144.0s | 11520.9MB] Final cluster 5 threshold=6.557305 count=44714\n[ 1144.0s | 11520.9MB] POS_WEIGHT full=13.7304\n[ 1198.3s | 11695.3MB] \n===============================\nInference on Test Set\n===============================\n[ 1206.6s | 11566.6MB] hybrid_test: shape=(138820,), dtype=float64, min=-0.4199, max=40.7087, mean=0.0000, std=0.9726, NaNs=0\n[ 1227.6s | 11586.2MB] meta_X_test: shape=(138820, 5), dtype=float64, min=-41.1722, max=248.8928, mean=-4.3580, std=11.8829, NaNs=0\n[ 1228.0s | 11586.2MB] [AE per-cluster] Acc=0.9529 Prec=0.9158 Rec=0.3380 F1=0.4937 AP=0.5816 AUC=0.9528\n[ 1228.3s | 11586.2MB] [Meta Ensemble] Acc=0.9970 Prec=0.9570 Rec=1.0000 F1=0.9780 AP=1.0000 AUC=1.0000\n[ 1228.6s | 11586.2MB] Classification report (Meta):\n              precision    recall  f1-score   support\n\n      Normal       1.00      1.00      1.00    129396\n      Attack       0.96      1.00      0.98      9424\n\n    accuracy                           1.00    138820\n   macro avg       0.98      1.00      0.99    138820\nweighted avg       1.00      1.00      1.00    138820\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 600x500 with 2 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"[ 1228.8s | 11586.2MB] \n==============================\nBenchmark Comparison\n==============================\n[ 1229.0s | 11586.2MB] Final comparison table:\n            Method  Accuracy  Precision  Recall    F1\n        Paper (AE)     94.00      81.00    99.0 89.00\n      Fernandes RF     98.00      99.00    69.0 81.00\n     Fernandes XGB     96.00      99.00    44.0 61.00\n      Vitorino KNN     98.00      98.00    98.0 98.00\n      Vitorino MLP     90.00      90.00    90.0 89.00\nOur AE per-cluster     95.29      91.58    33.8 49.37\n Our Meta Ensemble     99.70      95.70   100.0 97.80\n\nNotes:\n- Adjust PR_TARGET_PRECISION to bias threshold toward higher precision.\n- Increase quantile (0.995) for cluster thresholds for stricter anomaly gating.\n- ENABLE_FLOW=True may add a new signal; watch logs for stability.\n- Reduce DEBUG_LEVEL to 1 or 0 for cleaner output once stable.\n- Use SMALL_SAMPLE to iterate faster on large dataset.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Validate Anomaly Detection Results (Kaggle-ready, single-cell friendly)\n# ---------------------------------------------------------------------\n# What it does:\n# - Loads previously computed meta probabilities and labels, or (optionally) builds a quick minimal pipeline.\n# - Runs a comprehensive statistical validation to check that results are leak-free, real, and robust:\n#   * Separation & KS tests, gap analysis, overlap counts\n#   * PR-based threshold, plateau (mid-gap) sweep\n#   * Bootstrap CIs for Precision/Recall/F1\n#   * Permutation test (leakage check) on OOF meta features\n#   * Feature ablation and per-feature shuffle impact\n#   * Label-noise sensitivity, perturbation robustness\n#   * Calibration (Brier, reliability curve)\n#   * Simple conformal normal-rejection test\n# - Saves a Markdown report + plots to /kaggle/working.\n#\n# How to use in Kaggle:\n# 1) If you already ran your training script in this notebook session and have:\n#       meta_prob_test, y_test, oof_X, oof_y, feature_names\n#    in memory (globals), this cell will detect and use them directly.\n# 2) Or, save artifacts as .npy in a dir and set ARTIFACTS_DIR below to load them:\n#       meta_prob_test.npy, y_test.npy, oof_X.npy, oof_y.npy, feature_names.json (optional)\n# 3) Or, set RUN_MINIMAL_PIPELINE=True and point CSV_PATH to run a quick (lighter) pipeline\n#    that produces meta_prob_test/oof features for validation (good for demos).\n#\n# Outputs:\n# - /kaggle/working/validation_outputs/validation_summary.json\n# - /kaggle/working/validation_outputs/probability_distributions.png\n# - /kaggle/working/validation_outputs/calibration_curve.png\n# - /kaggle/working/validation_outputs/plateau_sweep.png (if gap exists)\n# - /kaggle/working/results_validation_report.md (readable Markdown report)\n\nimport os, json, math\nfrom pathlib import Path\nfrom typing import Dict, Any, List\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib\nmatplotlib.use(\"Agg\")  # keep headless for Kaggle batch runs\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    roc_auc_score, average_precision_score, precision_recall_curve,\n    brier_score_loss\n)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.calibration import calibration_curve\nfrom scipy.stats import ks_2samp\n\n# =========================\n# Configuration (edit here)\n# =========================\nARTIFACTS_DIR = None             # e.g. \"/kaggle/working/artifacts\" (where meta_prob_test.npy etc. live). Set None to auto-detect from memory.\nCSV_PATH = None                  # e.g. \"/kaggle/input/hikari-dataset/ALLFLOWMETER_HIKARI2021.csv\" if RUN_MINIMAL_PIPELINE=True\nRUN_MINIMAL_PIPELINE = False     # Set True to build a quick minimal pipeline if no artifacts/memory vars are present.\n\nOUTPUT_DIR = \"/kaggle/working/validation_outputs\"\nREPORT_PATH = \"/kaggle/working/results_validation_report.md\"\n\nTARGET_PRECISION = 0.95          # PR-based thresholding target\nBOOTSTRAP_ITERS = 300\nPERMUTATION_REPEATS = 5\nNOISE_FRACS = [0.005, 0.01, 0.02, 0.05]\nEPSILON = 0.01                   # Small Gaussian noise for perturbation robustness\n\n# =========================\n# Utilities\n# =========================\ndef _exists_in_globals(name: str) -> bool:\n    return name in globals() and globals()[name] is not None\n\ndef compute_basic_metrics(y_true, y_pred, probs=None) -> Dict[str, float]:\n    out = {\n        \"accuracy\": accuracy_score(y_true, y_pred),\n        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n        \"f1\": f1_score(y_true, y_pred, zero_division=0),\n    }\n    if probs is not None:\n        out[\"auc\"] = roc_auc_score(y_true, probs)\n        out[\"ap\"] = average_precision_score(y_true, probs)\n    return out\n\ndef pr_threshold(scores, y_true, target_precision=None):\n    prec, rec, thr = precision_recall_curve(y_true, scores)\n    f1 = (2 * prec * rec) / (prec + rec + 1e-12)\n    best_idx = np.nanargmax(f1)\n    best_thr = thr[max(best_idx - 1, 0)]\n    best = dict(kind=\"max_f1\", threshold=float(best_thr),\n                precision=float(prec[best_idx]), recall=float(rec[best_idx]), f1=float(f1[best_idx]))\n    if target_precision is None:\n        return best\n    eligible = np.where(prec[:-1] >= target_precision)[0]\n    if eligible.size == 0:\n        return best\n    idx = eligible[np.argmax(rec[eligible])]\n    return dict(kind=f\"target_precision_{target_precision}\",\n                threshold=float(thr[idx]),\n                precision=float(prec[idx]),\n                recall=float(rec[idx]),\n                f1=float(f1[idx]))\n\ndef bootstrap_metric_ci(y_true, probs, threshold, B=300, seed=42):\n    rng = np.random.default_rng(seed)\n    n = len(y_true)\n    mets = []\n    for _ in range(B):\n        idx = rng.integers(0, n, n)\n        yb = y_true[idx]\n        sb = probs[idx]\n        pred = (sb >= threshold).astype(int)\n        mets.append([\n            precision_score(yb, pred, zero_division=0),\n            recall_score(yb, pred, zero_division=0),\n            f1_score(yb, pred, zero_division=0),\n        ])\n    arr = np.array(mets)\n    return {\n        \"precision_CI\": np.percentile(arr[:, 0], [2.5, 50, 97.5]).tolist(),\n        \"recall_CI\": np.percentile(arr[:, 1], [2.5, 50, 97.5]).tolist(),\n        \"f1_CI\": np.percentile(arr[:, 2], [2.5, 50, 97.5]).tolist(),\n    }\n\ndef permutation_test(oof_X, oof_y, repeats=5, seed=42):\n    rng = np.random.default_rng(seed)\n    base_model = LogisticRegression(max_iter=200, class_weight='balanced')\n    base_model.fit(oof_X, oof_y)\n    base_auc = roc_auc_score(oof_y, base_model.predict_proba(oof_X)[:, 1])\n\n    results = []\n    for r in range(repeats):\n        y_perm = rng.permutation(oof_y)\n        m = LogisticRegression(max_iter=200, class_weight='balanced').fit(oof_X, y_perm)\n        p = m.predict_proba(oof_X)[:, 1]\n        results.append({\"repeat\": r + 1,\n                        \"auc\": float(roc_auc_score(y_perm, p)),\n                        \"ap\": float(average_precision_score(y_perm, p))})\n    return base_auc, results\n\ndef feature_ablation(oof_X, oof_y, feature_names):\n    base_m = LogisticRegression(max_iter=200, class_weight='balanced').fit(oof_X, oof_y)\n    base_probs = base_m.predict_proba(oof_X)[:, 1]\n    base_auc = roc_auc_score(oof_y, base_probs)\n    base_ap = average_precision_score(oof_y, base_probs)\n\n    ablation = []\n    for i, fname in enumerate(feature_names):\n        keep = [j for j in range(oof_X.shape[1]) if j != i]\n        X_red = oof_X[:, keep]\n        m = LogisticRegression(max_iter=200, class_weight='balanced').fit(X_red, oof_y)\n        p = m.predict_proba(X_red)[:, 1]\n        auc = roc_auc_score(oof_y, p)\n        ap = average_precision_score(oof_y, p)\n        ablation.append({\n            \"removed_feature\": fname,\n            \"auc\": float(auc),\n            \"ap\": float(ap),\n            \"auc_drop\": float(base_auc - auc),\n            \"ap_drop\": float(base_ap - ap),\n        })\n\n    singles = []\n    for i, fname in enumerate(feature_names):\n        X_single = oof_X[:, [i]]\n        m = LogisticRegression(max_iter=200, class_weight='balanced').fit(X_single, oof_y)\n        p = m.predict_proba(X_single)[:, 1]\n        singles.append({\"feature\": fname,\n                        \"single_auc\": float(roc_auc_score(oof_y, p)),\n                        \"single_ap\": float(average_precision_score(oof_y, p))})\n    return {\"base_auc\": float(base_auc), \"base_ap\": float(base_ap),\n            \"ablation\": ablation, \"singles\": singles}\n\ndef feature_shuffle_impact(oof_X, oof_y, feature_names, seed=42):\n    rng = np.random.default_rng(seed)\n    base_m = LogisticRegression(max_iter=200, class_weight='balanced').fit(oof_X, oof_y)\n    base_auc = roc_auc_score(oof_y, base_m.predict_proba(oof_X)[:, 1])\n    out = []\n    for i, fname in enumerate(feature_names):\n        X_shuf = oof_X.copy()\n        rng.shuffle(X_shuf[:, i])\n        m = LogisticRegression(max_iter=200, class_weight='balanced').fit(X_shuf, oof_y)\n        auc = roc_auc_score(oof_y, m.predict_proba(X_shuf)[:, 1])\n        out.append({\"shuffled_feature\": fname,\n                    \"auc_after_shuffle\": float(auc),\n                    \"auc_delta\": float(auc - base_auc)})\n    return out\n\ndef label_noise_sensitivity(oof_X, oof_y, noise_fracs=[0.005,0.01,0.02,0.05], seed=42):\n    rng = np.random.default_rng(seed)\n    results = []\n    for frac in noise_fracs:\n        y_noisy = oof_y.copy()\n        k = int(len(y_noisy) * frac)\n        idx = rng.choice(len(y_noisy), k, replace=False)\n        y_noisy[idx] = 1 - y_noisy[idx]\n        m = LogisticRegression(max_iter=200, class_weight='balanced').fit(oof_X, y_noisy)\n        p = m.predict_proba(oof_X)[:, 1]\n        results.append({\"noise_fraction\": float(frac),\n                        \"AUC_vs_true\": float(roc_auc_score(oof_y, p)),\n                        \"AP_vs_true\": float(average_precision_score(oof_y, p))})\n    return results\n\ndef perturbation_robustness(meta_probs, y_test, epsilon=0.01, repeats=5, seed=42):\n    rng = np.random.default_rng(seed)\n    metrics = []\n    for r in range(repeats):\n        noise = rng.normal(0, epsilon, size=meta_probs.shape)\n        pert = np.clip(meta_probs + noise, 0, 1)\n        pred = (pert >= 0.5).astype(int)  # generic sanity check threshold\n        m = compute_basic_metrics(y_test, pred, pert)\n        metrics.append({\"repeat\": r + 1, **{k: float(v) for k, v in m.items()}})\n    return metrics\n\ndef ks_and_overlap(meta_probs, y_test):\n    normal_scores = meta_probs[y_test == 0]\n    attack_scores = meta_probs[y_test == 1]\n    max_normal = float(normal_scores.max())\n    min_attack = float(attack_scores.min())\n    gap_width = float(min_attack - max_normal)\n    overlap_count = int(np.sum(attack_scores <= max_normal))\n    ks_stat, ks_p = ks_2samp(normal_scores, attack_scores)\n    return {\"max_normal\": max_normal,\n            \"min_attack\": min_attack,\n            \"gap_width\": gap_width,\n            \"overlap_attack_leq_max_normal\": overlap_count,\n            \"total_attacks\": int(len(attack_scores)),\n            \"ks_stat\": float(ks_stat),\n            \"ks_p_value\": float(ks_p)}\n\ndef calibration_metrics(meta_probs, y_test, n_bins=20):\n    prob_true, prob_pred = calibration_curve(y_test, meta_probs, n_bins=n_bins, strategy='uniform')\n    brier = brier_score_loss(y_test, meta_probs)\n    # Approximate ECE as average |pred - true| across bins\n    ece = float(np.mean(np.abs(prob_pred - prob_true))) if len(prob_true) else float('nan')\n    return {\"brier_score\": float(brier),\n            \"ece\": ece,\n            \"calibration_points\": [{\"mean_pred\": float(pd), \"fraction_pos\": float(pt)}\n                                   for pt, pd in zip(prob_true, prob_pred)]}\n\ndef plateau_threshold_sweep(meta_probs, y_test, gap_info, num_points=20):\n    if gap_info[\"gap_width\"] <= 0:\n        return []\n    lo = gap_info[\"max_normal\"] + 1e-6\n    hi = gap_info[\"min_attack\"] - 1e-6\n    ths = np.linspace(lo, hi, num_points)\n    out = []\n    for t in ths:\n        pred = (meta_probs >= t).astype(int)\n        m = compute_basic_metrics(y_test, pred, meta_probs)\n        out.append({\"threshold\": float(t), **{k: float(v) for k, v in m.items()}})\n    return out\n\ndef conformal_normal_rejection(meta_probs, y_test, alpha=0.01):\n    normal_scores = meta_probs[y_test == 0]\n    attack_scores = meta_probs[y_test == 1]\n    q_alpha = float(np.quantile(normal_scores, 1 - alpha))\n    flagged = int(np.sum(attack_scores >= q_alpha))\n    rate = float(flagged / max(1, len(attack_scores)))\n    return {\"alpha\": alpha,\n            \"normal_nonconformity_quantile\": q_alpha,\n            \"attacks_flagged_at_alpha\": flagged,\n            \"total_attacks\": int(len(attack_scores)),\n            \"attack_flag_rate\": rate}\n\ndef generate_markdown_report(path: Path, summary: Dict[str, Any]):\n    lines = []\n    a = lines.append\n\n    a(\"# Validation Report: Anomaly Detection MetaEnsemble\")\n    a(\"\")\n    a(\"## Separation & Distribution\")\n    g = summary[\"gap_info\"]\n    a(f\"- Max normal prob: **{g['max_normal']:.6f}**\")\n    a(f\"- Min attack prob: **{g['min_attack']:.6f}**\")\n    a(f\"- Gap width: **{g['gap_width']:.6f}**\")\n    a(f\"- Overlapping attacks (<= max normal): **{g['overlap_attack_leq_max_normal']}/{g['total_attacks']}**\")\n    a(f\"- KS: **{g['ks_stat']:.4f}**, pvalue: **{g['ks_p_value']:.2e}**\")\n    a(\"\")\n    a(\"## PR Threshold & Plateau\")\n    a(f\"- PR threshold (target precision): {summary['pr_threshold']}\")\n    if summary[\"plateau\"]:\n        good = all(p['precision']==1.0 and p['recall']==1.0 for p in summary[\"plateau\"])\n        a(f\"- Plateau points: {len(summary['plateau'])}, perfect across plateau: {good}\")\n    else:\n        a(\"- No gap plateau (overlap present or not computed).\")\n    a(\"\")\n    a(\"## Bootstrap Confidence Intervals\")\n    ci = summary[\"bootstrap_ci\"]\n    a(f\"- Precision CI: {ci['precision_CI']}\")\n    a(f\"- Recall CI: {ci['recall_CI']}\")\n    a(f\"- F1 CI: {ci['f1_CI']}\")\n    a(\"\")\n    if \"permutation\" in summary and summary[\"permutation\"] is not None:\n        a(\"## Permutation Test (Leakage Check)\")\n        base_auc = summary[\"permutation\"][\"base_auc\"]\n        avg_perm_auc = float(np.mean([r['auc'] for r in summary[\"permutation\"][\"results\"]]))\n        a(f\"- Base AUC: {base_auc:.4f}\")\n        a(f\"- Mean permutation AUC: {avg_perm_auc:.4f}\")\n        a(f\"- Repeats: {summary['permutation']['results']}\")\n        a(\"\")\n    if \"feature_ablation\" in summary and summary[\"feature_ablation\"] is not None:\n        a(\"## Feature Ablation & SingleFeature Performance\")\n        abl = summary[\"feature_ablation\"]\n        a(f\"- Base AUC/AP: {abl['base_auc']:.4f} / {abl['base_ap']:.4f}\")\n        drops = sorted(abl[\"ablation\"], key=lambda x: x[\"auc_drop\"], reverse=True)[:5]\n        a(\"- Top AUC drops on removal:\")\n        for d in drops:\n            a(f\"  * {d['removed_feature']}: AUC={d['auc_drop']:.4f}, AP={d['ap_drop']:.4f}\")\n        singles = sorted(abl[\"singles\"], key=lambda x: x[\"single_auc\"], reverse=True)[:5]\n        a(\"- Top singlefeature AUC/AP:\")\n        for s in singles:\n            a(f\"  * {s['feature']}: AUC={s['single_auc']:.4f}, AP={s['single_ap']:.4f}\")\n        a(\"\")\n    if \"feature_shuffle\" in summary and summary[\"feature_shuffle\"] is not None:\n        a(\"## Feature Shuffle Impact\")\n        worst = sorted(summary[\"feature_shuffle\"], key=lambda x: x[\"auc_delta\"])[:5]\n        for w in worst:\n            a(f\"- {w['shuffled_feature']}: AUC delta {w['auc_delta']:.4f}\")\n        a(\"\")\n    a(\"## Label Noise Sensitivity\")\n    for row in summary[\"label_noise\"]:\n        a(f\"- Noise {row['noise_fraction']:.3f}: AUC={row['AUC_vs_true']:.4f}, AP={row['AP_vs_true']:.4f}\")\n    a(\"\")\n    a(\"## Perturbation Robustness\")\n    for r in summary[\"perturbation\"]:\n        a(f\"- Repeat {r['repeat']}: Acc={r['accuracy']:.4f}, Prec={r['precision']:.4f}, Rec={r['recall']:.4f}, F1={r['f1']:.4f}\")\n    a(\"\")\n    a(\"## Calibration\")\n    c = summary[\"calibration\"]\n    a(f\"- Brier: {c['brier_score']:.6e}, ECE{c['ece']:.6f}\")\n    a(\"  (See calibration_curve.png)\")\n    a(\"\")\n    a(\"## Conformal NormalRejection (=0.01)\")\n    conf = summary[\"conformal\"]\n    a(f\"- Normal nonconformity (1-) quantile: {conf['normal_nonconformity_quantile']:.6f}\")\n    a(f\"- Attack flag rate at : {conf['attack_flag_rate']:.4f}\")\n    a(\"\")\n    a(\"## Overall Validity (Heuristics)\")\n    verdicts = []\n    if g['gap_width'] > 0 and g['overlap_attack_leq_max_normal'] == 0:\n        verdicts.append(\"Perfect ranking gap (supports AUC/AP=1).\")\n    if c['brier_score'] < 1e-4:\n        verdicts.append(\"Nearzero Brier (wellcalibrated extremes).\")\n    if \"permutation\" in summary and summary[\"permutation\"] is not None:\n        avg_perm_auc = float(np.mean([r['auc'] for r in summary[\"permutation\"][\"results\"]]))\n        if avg_perm_auc < 0.55:\n            verdicts.append(\"Permutation test ~chance (no leakage).\")\n    a(\"- \" + \" | \".join(verdicts) if verdicts else \"- Mixed signals; consider further leak scan or external validation.\")\n    path.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n\n# =========================\n# Optional: Minimal pipeline (fast demo if needed)\n# =========================\ndef run_minimal_pipeline(csv_path: str, sample_size=None, seed=42):\n    df = pd.read_csv(csv_path)\n    if sample_size and len(df) > sample_size:\n        df = df.sample(sample_size, random_state=seed).reset_index(drop=True)\n\n    # Detect label col\n    label_col = None\n    for c in ['label','Label','LABEL','class','Class','attack','Attack','type','Type','TYPE']:\n        if c in df.columns:\n            label_col = c; break\n    if label_col is None: label_col = df.columns[-1]\n    if label_col != 'label':\n        df.rename(columns={label_col: 'label'}, inplace=True)\n    s = df['label'].astype(str).str.lower().str.strip()\n    is_normal = None\n    for kw in ['normal','benign','legitimate','background']:\n        if s.str.contains(kw).any():\n            is_normal = s.str.contains(kw); break\n    if is_normal is None:\n        is_normal = (df['label'] == df['label'].value_counts().idxmax())\n    df['is_attack'] = (~is_normal).astype(int)\n\n    # Simple numeric features excluding obvious leaks\n    feats = []\n    for c in df.columns:\n        lc = c.lower()\n        if c in ['label','is_attack']: continue\n        if any(p in lc for p in ['label','ip','port','time','mac','src','dst','flow','address','id']): continue\n        if df[c].dtype not in [np.int16,np.int32,np.int64,np.float16,np.float32,np.float64]: continue\n        if df[c].nunique() <= 1: continue\n        feats.append(c)\n    X = df[feats].replace([np.inf,-np.inf], np.nan).fillna(method='ffill').fillna(0).values\n    y = df['is_attack'].values\n\n    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=seed,stratify=y)\n    scaler = StandardScaler().fit(X_train)\n    X_train_s = scaler.transform(X_train)\n    X_test_s = scaler.transform(X_test)\n\n    # Detectors: IsolationForest + LR + XGB\n    iso = IsolationForest(n_estimators=200, random_state=seed, contamination='auto').fit(X_train_s)\n    iso_val = None  # build OOF below\n    # Fast 3-fold OOF for meta\n    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=seed)\n    oof_feats, oof_labels = [], []\n    for tr, va in skf.split(X_train_s, y_train):\n        X_tr, X_va = X_train_s[tr], X_train_s[va]; y_tr, y_va = y_train[tr], y_train[va]\n        iso_f = IsolationForest(n_estimators=200, random_state=seed, contamination='auto').fit(X_tr)\n        iso_va = -iso_f.score_samples(X_va)\n        lr_f = LogisticRegression(max_iter=300, class_weight='balanced').fit(X_tr, y_tr)\n        lr_va = lr_f.predict_proba(X_va)[:,1]\n        try:\n            import xgboost as xgb\n            xgb_f = xgb.XGBClassifier(\n                n_estimators=300,max_depth=8,learning_rate=0.05,\n                subsample=0.8,colsample_bytree=0.8,\n                scale_pos_weight=(y_tr==0).sum()/max(1,(y_tr==1).sum()),\n                objective=\"binary:logistic\", eval_metric=\"logloss\", random_state=seed\n            ).fit(X_tr,y_tr)\n            xgb_va = xgb_f.predict_proba(X_va)[:,1]\n        except Exception:\n            xgb_va = np.zeros_like(lr_va)\n        F = np.vstack([iso_va, lr_va, xgb_va]).T\n        oof_feats.append(F); oof_labels.append(y_va)\n    oof_X = np.vstack(oof_feats); oof_y = np.concatenate(oof_labels)\n\n    # Final meta on OOF; score test\n    meta = LogisticRegression(max_iter=300, class_weight='balanced').fit(oof_X, oof_y)\n    iso_test = -iso.score_samples(X_test_s)\n    lr = LogisticRegression(max_iter=300, class_weight='balanced').fit(X_train_s, y_train)\n    lr_test = lr.predict_proba(X_test_s)[:,1]\n    try:\n        import xgboost as xgb\n        xgb_model = xgb.XGBClassifier(\n            n_estimators=300,max_depth=8,learning_rate=0.05,\n            subsample=0.8,colsample_bytree=0.8,\n            scale_pos_weight=(y_train==0).sum()/max(1,(y_train==1).sum()),\n            objective=\"binary:logistic\", eval_metric=\"logloss\", random_state=seed\n        ).fit(X_train_s, y_train)\n        xgb_test = xgb_model.predict_proba(X_test_s)[:,1]\n    except Exception:\n        xgb_test = np.zeros_like(lr_test)\n\n    meta_X_test = np.vstack([iso_test, lr_test, xgb_test]).T\n    meta_prob_test = meta.predict_proba(meta_X_test)[:,1]\n    feature_names = [\"iso_score\",\"lr_prob\",\"xgb_prob\"]\n    return {\"meta_prob_test\": meta_prob_test,\n            \"y_test\": y_test,\n            \"oof_X\": oof_X,\n            \"oof_y\": oof_y,\n            \"feature_names\": feature_names}\n\n# =========================\n# Driver (no CLI; Kaggle cell friendly)\n# =========================\n# 1) Gather artifacts\nartifacts = {}\n\nif _exists_in_globals(\"meta_prob_test\") and _exists_in_globals(\"y_test\"):\n    # Use in-memory outputs from previous cell\n    artifacts[\"meta_prob_test\"] = globals()[\"meta_prob_test\"]\n    artifacts[\"y_test\"] = globals()[\"y_test\"]\n    artifacts[\"oof_X\"] = globals()[\"oof_X\"] if _exists_in_globals(\"oof_X\") else None\n    artifacts[\"oof_y\"] = globals()[\"oof_y\"] if _exists_in_globals(\"oof_y\") else None\n    artifacts[\"feature_names\"] = globals()[\"feature_names\"] if _exists_in_globals(\"feature_names\") else None\nelif ARTIFACTS_DIR:\n    # Load from disk\n    d = Path(ARTIFACTS_DIR)\n    artifacts[\"meta_prob_test\"] = np.load(d / \"meta_prob_test.npy\")\n    artifacts[\"y_test\"] = np.load(d / \"y_test.npy\")\n    artifacts[\"oof_X\"] = np.load(d / \"oof_X.npy\") if (d / \"oof_X.npy\").exists() else None\n    artifacts[\"oof_y\"] = np.load(d / \"oof_y.npy\") if (d / \"oof_y.npy\").exists() else None\n    if (d / \"feature_names.json\").exists():\n        artifacts[\"feature_names\"] = json.loads((d / \"feature_names.json\").read_text())\n    else:\n        artifacts[\"feature_names\"] = [f\"feat_{i}\" for i in range(artifacts[\"oof_X\"].shape[1])] if artifacts[\"oof_X\"] is not None else None\nelif RUN_MINIMAL_PIPELINE and CSV_PATH:\n    # Build quick artifacts\n    artifacts = run_minimal_pipeline(CSV_PATH, sample_size=None)\nelse:\n    raise RuntimeError(\n        \"No inputs found. Provide in-memory arrays (meta_prob_test, y_test), \"\n        \"or set ARTIFACTS_DIR to load *.npy, or enable RUN_MINIMAL_PIPELINE with CSV_PATH.\"\n    )\n\nmeta_prob_test = artifacts[\"meta_prob_test\"]\ny_test = artifacts[\"y_test\"]\noof_X = artifacts.get(\"oof_X\", None)\noof_y = artifacts.get(\"oof_y\", None)\nfeature_names = artifacts.get(\"feature_names\", None)\nif (oof_X is not None) and (feature_names is None):\n    feature_names = [f\"feat_{i}\" for i in range(oof_X.shape[1])]\n\n# 2) Compute PR threshold and predictions\nthr_info = pr_threshold(meta_prob_test, y_test, target_precision=TARGET_PRECISION)\nthr = thr_info[\"threshold\"]\npred = (meta_prob_test >= thr).astype(int)\nbase_metrics = compute_basic_metrics(y_test, pred, meta_prob_test)\n\n# 3) Gap/overlap & KS\ngap_info = ks_and_overlap(meta_prob_test, y_test)\n\n# 4) Plateau sweep (if gap)\nplateau = plateau_threshold_sweep(meta_prob_test, y_test, gap_info, num_points=25) if gap_info[\"gap_width\"] > 0 else []\n\n# 5) Bootstrap CI\nbootstrap_ci = bootstrap_metric_ci(y_test, meta_prob_test, thr, B=BOOTSTRAP_ITERS)\n\n# 6) Permutation / Ablation / Shuffle Impact (only if OOF available)\nperm_summary = None\nablation_summary = None\nshuffle_summary = None\nif (oof_X is not None) and (oof_y is not None):\n    base_auc, perm_results = permutation_test(oof_X, oof_y, repeats=PERMUTATION_REPEATS)\n    perm_summary = {\"base_auc\": float(base_auc), \"results\": perm_results}\n    ablation_summary = feature_ablation(oof_X, oof_y, feature_names)\n    shuffle_summary = feature_shuffle_impact(oof_X, oof_y, feature_names)\n\n# 7) Label noise sensitivity (if OOF)\nnoise_summary = label_noise_sensitivity(oof_X, oof_y, NOISE_FRACS) if (oof_X is not None) else []\n\n# 8) Perturbation robustness\npert_summary = perturbation_robustness(meta_prob_test, y_test, epsilon=EPSILON, repeats=5)\n\n# 9) Calibration\ncalib_summary = calibration_metrics(meta_prob_test, y_test)\n\n# 10) Conformal\nconf_summary = conformal_normal_rejection(meta_prob_test, y_test, alpha=0.01)\n\n# 11) Save artifacts & plots\nout_dir = Path(OUTPUT_DIR); out_dir.mkdir(parents=True, exist_ok=True)\n\n# Probability histograms\nplt.figure(figsize=(9,4))\nsns.histplot(meta_prob_test[y_test==0], bins=50, color='steelblue', stat='density', alpha=0.6, label='Normal')\nsns.histplot(meta_prob_test[y_test==1], bins=50, color='darkorange', stat='density', alpha=0.6, label='Attack')\nplt.axvline(thr, color='red', linestyle='--', label=f'PR thr {thr:.4f}')\nplt.legend(); plt.title(\"Meta Probability Distributions\")\nplt.tight_layout(); plt.savefig(out_dir / \"probability_distributions.png\"); plt.close()\n\n# Calibration curve\npts = calib_summary[\"calibration_points\"]\nplt.figure(figsize=(4,4))\nif pts:\n    xs = [p[\"mean_pred\"] for p in pts]; ys = [p[\"fraction_pos\"] for p in pts]\n    plt.plot(xs, ys, marker='o')\nplt.plot([0,1],[0,1],'--', color='gray')\nplt.xlabel(\"Mean predicted\"); plt.ylabel(\"Fraction positives\"); plt.title(\"Calibration (Uniform bins)\")\nplt.tight_layout(); plt.savefig(out_dir / \"calibration_curve.png\"); plt.close()\n\n# Plateau plot\nif plateau:\n    plt.figure(figsize=(6,4))\n    ths = [p[\"threshold\"] for p in plateau]\n    precs = [p[\"precision\"] for p in plateau]\n    recs = [p[\"recall\"] for p in plateau]\n    plt.plot(ths, precs, label=\"Precision\")\n    plt.plot(ths, recs, label=\"Recall\")\n    plt.ylim(0.0, 1.05)\n    plt.xlabel(\"Threshold (inside gap)\"); plt.ylabel(\"Metric\")\n    plt.title(\"Plateau Sweep (Gap Region)\")\n    plt.legend(); plt.tight_layout()\n    plt.savefig(out_dir / \"plateau_sweep.png\"); plt.close()\n\n# 12) Build summary, write JSON + Markdown\nsummary = {\n    \"base_metrics_at_PR_threshold\": {k: float(v) for k, v in base_metrics.items()},\n    \"pr_threshold\": thr_info,\n    \"gap_info\": gap_info,\n    \"plateau\": plateau,\n    \"bootstrap_ci\": bootstrap_ci,\n    \"permutation\": perm_summary,\n    \"feature_ablation\": ablation_summary,\n    \"feature_shuffle\": shuffle_summary,\n    \"label_noise\": noise_summary,\n    \"perturbation\": pert_summary,\n    \"calibration\": calib_summary,\n    \"conformal\": conf_summary,\n}\n(Path(OUTPUT_DIR) / \"validation_summary.json\").write_text(json.dumps(summary, indent=2))\n\ngenerate_markdown_report(Path(REPORT_PATH), summary)\n\nprint(\"Validation complete.\")\nprint(f\"- PR threshold: {thr_info}\")\nprint(f\"- Base metrics @ PR thr: {base_metrics}\")\nprint(f\"- Gap info: {gap_info}\")\nprint(f\"- Bootstrap CIs: {bootstrap_ci}\")\nif perm_summary: print(f\"- Permutation mean AUC: {np.mean([r['auc'] for r in perm_summary['results']]):.4f}\")\nprint(f\"Report: {REPORT_PATH}\")\nprint(f\"Artifacts: {OUTPUT_DIR}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T16:06:44.323199Z","iopub.execute_input":"2025-11-16T16:06:44.323985Z","iopub.status.idle":"2025-11-16T16:08:25.587563Z","shell.execute_reply.started":"2025-11-16T16:06:44.323960Z","shell.execute_reply":"2025-11-16T16:08:25.586829Z"}},"outputs":[{"name":"stdout","text":"Validation complete.\n- PR threshold: {'kind': 'target_precision_0.95', 'threshold': 0.9877002452361338, 'precision': 1.0, 'recall': 0.00010611205432937182, 'f1': 0.00021220159151172415}\n- Base metrics @ PR thr: {'accuracy': 0.9321207318830139, 'precision': 1.0, 'recall': 0.00010611205432937182, 'f1': 0.00021220159151193635, 'auc': 0.9438252874357712, 'ap': 0.43207149892000307}\n- Gap info: {'max_normal': 0.9876614483135047, 'min_attack': 0.04058433171431679, 'gap_width': -0.9470771165991879, 'overlap_attack_leq_max_normal': 9423, 'total_attacks': 9424, 'ks_stat': 0.8507349164284828, 'ks_p_value': 0.0}\n- Bootstrap CIs: {'precision_CI': [0.0, 1.0, 1.0], 'recall_CI': [0.0, 0.00010599957719269772, 0.00042190539935517386], 'f1_CI': [0.0, 0.00021197668494616397, 0.0008434549404537036]}\n- Permutation mean AUC: 0.5035\nReport: /kaggle/working/results_validation_report.md\nArtifacts: /kaggle/working/validation_outputs\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}